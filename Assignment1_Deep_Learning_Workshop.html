<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>9e61992f799741639cca65f2b21e9119</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell markdown" id="SOMw3OXgBEeu">
<p>Elad Inbar - 205358211 Ido Livne - 203834733</p>
</div>
<div class="cell markdown" id="KUjRH52Bc1G-">
<p>Our work will be using the dataset:</p>
<p>â€¢ <a href="https://www.kaggle.com/jessicali9530/stanford-cars-dataset" class="uri">https://www.kaggle.com/jessicali9530/stanford-cars-dataset</a></p>
</div>
<section id="import" class="cell markdown" id="xPAltdwIEuAs">
<h1><strong>Import</strong></h1>
</section>
<div class="cell code" data-execution_count="1" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:6982,&quot;timestamp&quot;:1670963261030}" id="iO3R1Ek0Gk7u">
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download dataset and initialize runtime</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define Image Paths</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Exploratory Data Analysis</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Pre-Processing</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.io <span class="im">as</span> sio </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> ToTensor</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.io <span class="im">import</span> read_image</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms.functional <span class="im">import</span> resize</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imgaug <span class="im">import</span> augmenters <span class="im">as</span> iaa</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Forming the neural network</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding Inference-Time-Augmentation</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># from tqdm import tqdm</span></span></code></pre></div>
<div class="output stream stderr">
<pre><code>/home/eladin/.conda/envs/my_env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</code></pre>
</div>
</div>
<section id="download-dataset-and-initialize-runtime" class="cell markdown" id="Qxi39cWdGR42">
<h1><strong>Download dataset and initialize runtime</strong></h1>
</section>
<div class="cell code" data-execution_count="2" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:453,&quot;timestamp&quot;:1670963264921}" id="Pnv-HOU3Gv6j" data-outputId="b4e6cabb-efac-4710-969a-da45fe9d39d1" data-tags="[]">
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&#39;cuda&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>device</span></code></pre></div>
<div class="output execute_result" data-execution_count="2">
<pre><code>device(type=&#39;cuda&#39;)</code></pre>
</div>
</div>
<div class="cell code" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:3340,&quot;timestamp&quot;:1670963268258}" id="Hbu7ZmVcDgrD" data-tags="[]">
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>q kaggle</span></code></pre></div>
</div>
<div class="cell code" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:7,&quot;timestamp&quot;:1670963268259}" id="5WiQK49XEWGi">
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir <span class="op">~/</span>.kaggle</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:23777,&quot;timestamp&quot;:1670963292030}" id="LRaRH3xLr6iP" data-outputId="9b509f7f-27bb-4358-87f7-86efc3ef1ebf">
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>drive.mount(<span class="st">&#39;/content/drive&#39;</span>)</span></code></pre></div>
</div>
<div class="cell code" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:2723,&quot;timestamp&quot;:1670963294748}" id="XAvK7i-Yszth">
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>cp <span class="op">-</span>r <span class="st">&quot;/content/drive/My Drive/Deep Learning/kaggle.json&quot;</span> <span class="st">&quot;/content&quot;</span></span></code></pre></div>
</div>
<div class="cell code" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:6,&quot;timestamp&quot;:1670963294748}" id="zUKBvX5QEaFY">
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>cp kaggle.json <span class="op">~/</span>.kaggle<span class="op">/</span></span></code></pre></div>
</div>
<div class="cell code" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:7,&quot;timestamp&quot;:1670963294749}" id="KUp2zOvdEdG7">
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>chmod <span class="dv">600</span> <span class="op">~/</span>.kaggle<span class="op">/</span>kaggle.json</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:96662,&quot;timestamp&quot;:1670963391405}" id="1tgXk1xiEnvn" data-outputId="66496131-19bd-4305-9fec-aa87f53dc46d">
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>kaggle datasets download <span class="op">-</span>d jessicali9530<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset <span class="op">-</span>p content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span> <span class="op">--</span>unzip</span></code></pre></div>
</div>
<div class="cell code" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:37736,&quot;timestamp&quot;:1670963429118}" id="xuCpqsQAKM4r">
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>cp <span class="op">-</span>r <span class="st">&quot;/content/drive/My Drive/Deep Learning/cars_train_extra&quot;</span> <span class="st">&quot;/content/stanford-cars-dataset&quot;</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>cp <span class="op">-</span>r <span class="st">&quot;/content/drive/My Drive/Deep Learning/devkit&quot;</span> <span class="st">&quot;/content/stanford-cars-dataset&quot;</span></span></code></pre></div>
</div>
<div class="cell markdown" id="xryC-cU42zKx">
<p>Here we create a new directory that will include the original cropped samples. It will also be the default path for the dataset object.</p>
</div>
<div class="cell code" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:538,&quot;timestamp&quot;:1670963429644}" id="XYpCE22bpDvQ">
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cropped train image folder</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span>cropped_train</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span>cropped_train<span class="op">/</span>cropped_train</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Cropped test image folder</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span>cropped_test</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span>cropped_test<span class="op">/</span>cropped_test</span></code></pre></div>
</div>
<div class="cell markdown" id="eqyCKklFpv2A">
<p>Here we create a new directory to include the augmented (processed) images. We will use this directory to store the original cropped samples along with their augmentations, which will serve us during our 'Third Try' (train samples) and during Inference-Time-Augmentation (test samples).</p>
</div>
<div class="cell code" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:498,&quot;timestamp&quot;:1670963430140}" id="DWNsTNJ63c-3">
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Processed train image folder</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span>processed_train</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span>processed_train<span class="op">/</span>processed_train</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Processed test image folder</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span>processed_test</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span>processed_test<span class="op">/</span>processed_test</span></code></pre></div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extra train image folder</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span>cars_train_extra</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span>cars_train_extra<span class="op">/</span>cars_train_extra<span class="op">/</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Extra test image folder</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir <span class="op">/</span>content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span>cars_test_extra</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir <span class="op">/</span>content<span class="op">/</span>stanford<span class="op">-</span>cars<span class="op">-</span>dataset<span class="op">/</span>cars_test_extra<span class="op">/</span>cars_test_extra</span></code></pre></div>
</div>
<section id="define-image-paths" class="cell markdown" id="1ULzAhl1Gf9j">
<h1><strong>Define image paths</strong></h1>
</section>
<div class="cell code" data-execution_count="3" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:14,&quot;timestamp&quot;:1670963430142}" id="JbvaFqzOHdMB">
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>train_annos_path <span class="op">=</span> <span class="st">&#39;content/stanford-cars-dataset/devkit/cars_train_annos.mat&#39;</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>test_annos_path <span class="op">=</span> <span class="st">&#39;content/stanford-cars-dataset/devkit/cars_test_annos_withlabels.mat&#39;</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>train_path <span class="op">=</span> <span class="st">&#39;content/stanford-cars-dataset/cars_train/cars_train/&#39;</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>test_path <span class="op">=</span> <span class="st">&#39;content/stanford-cars-dataset/cars_test/cars_test/&#39;</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>extra_train_path <span class="op">=</span> <span class="st">&#39;content/stanford-cars-dataset/cars_train_extra/cars_train_extra/&#39;</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>extra_test_path <span class="op">=</span> <span class="st">&#39;content/stanford-cars-dataset/cars_test_extra/cars_test_extra/&#39;</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>processed_train_path <span class="op">=</span> <span class="st">&#39;content/stanford-cars-dataset/processed_train/processed_train/&#39;</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>processed_test_path <span class="op">=</span> <span class="st">&#39;content/stanford-cars-dataset/processed_test/processed_test/&#39;</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>cropped_train_path <span class="op">=</span> <span class="st">&#39;content/stanford-cars-dataset/cropped_train/cropped_train/&#39;</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>cropped_test_path <span class="op">=</span> <span class="st">&#39;content/stanford-cars-dataset/cropped_test/cropped_test/&#39;</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>y_train_labels_path <span class="op">=</span> <span class="st">&#39;content/stanford-cars-dataset/y_train_labels.csv&#39;</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>y_test_labels_path <span class="op">=</span> <span class="st">&#39;content/stanford-cars-dataset/y_test_labels.csv&#39;</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>train_dir <span class="op">=</span> <span class="bu">sorted</span>(os.listdir(train_path))</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>test_dir <span class="op">=</span> <span class="bu">sorted</span>(os.listdir(test_path))</span></code></pre></div>
</div>
<section id="exploratory-data-analysis" class="cell markdown" id="30-rkfrMJdIm">
<h1><strong>Exploratory Data Analysis</strong></h1>
<p>After downloading the dataset we can check how much data we're working with:</p>
</section>
<div class="cell code" data-execution_count="4" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:15,&quot;timestamp&quot;:1670963430143}" id="QAprCPwcH5-G" data-outputId="cd6715d5-9d51-4680-bc22-e9aa4e4e0e9c">
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Training data samples: &quot;</span>, <span class="bu">len</span>(train_dir))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Testing data samples: &quot;</span>, <span class="bu">len</span>(test_dir))</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Entire dataset samples: &quot;</span>, <span class="bu">len</span>(train_dir) <span class="op">+</span> <span class="bu">len</span>(test_dir))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Training data samples:  8144
Testing data samples:  8041
Entire dataset samples:  16185
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="5" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:11,&quot;timestamp&quot;:1670963430143}" id="txxpAxDbJNxc">
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_img(path):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> cv2.imread(path)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image[...,::<span class="op">-</span><span class="dv">1</span>]</span></code></pre></div>
</div>
<div class="cell markdown" id="JaNr7QlRKf5O">
<p>We can now test the dimensions of our images.</p>
</div>
<div class="cell code" data-execution_count="6" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:11,&quot;timestamp&quot;:1670963430144}" id="nRdJjEtTKVGC" data-outputId="bb526492-f3c6-42e5-fce0-ee69b9a15156">
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Image &#39;</span><span class="sc">{</span>train_dir[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">&#39; has the following shape: </span><span class="sc">{</span>load_img(train_path <span class="op">+</span> train_dir[<span class="dv">0</span>])<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Image &#39;</span><span class="sc">{</span>train_dir[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">&#39; has the following shape: </span><span class="sc">{</span>load_img(train_path <span class="op">+</span> train_dir[<span class="dv">1</span>])<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Image &#39;00001.jpg&#39; has the following shape: (400, 600, 3)
Image &#39;00002.jpg&#39; has the following shape: (675, 900, 3)
</code></pre>
</div>
</div>
<div class="cell markdown" id="CoMFw4YQLx5D">
<p>From these results we can conclude that the dimensions of each sample are different and that we are working with RGB type images.</p>
<p>Therefore, our data will require some pre-processing to streamline our input so that our model will be able to work and adapt to a fixed image size.</p>
<p>The data is also in .jpg format, so we will first have to convert the images to tensors that our model will know how to work with.</p>
</div>
<div class="cell markdown" id="brcbpJ5VQAU9">
<p>We would now like to obtain the different classes our data will belong to and figure out how many classes our model will have to get acquainted with:</p>
</div>
<div class="cell code" data-execution_count="7" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:10,&quot;timestamp&quot;:1670963430144}" id="BoS9U1nmMv91" data-outputId="a8ff231f-df83-4eff-f514-d24c5195d0e9">
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_classes():</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  cars_meta <span class="op">=</span> sio.loadmat(<span class="st">&#39;content/stanford-cars-dataset/devkit/cars_meta.mat&#39;</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> cars_meta[<span class="st">&#39;class_names&#39;</span>][<span class="dv">0</span>]</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> [class_list[<span class="dv">0</span>] <span class="cf">for</span> class_list <span class="kw">in</span> get_classes()]</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Classes size: &quot;</span>, <span class="bu">len</span>(classes))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Classes size:  196
</code></pre>
</div>
</div>
<div class="cell markdown" id="qaxytO8cRXeb">
<p>Classes are typically defined as 'Make Model Year'.</p>
<p>So for example a class can be defined as following:</p>
</div>
<div class="cell code" data-execution_count="8" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:9,&quot;timestamp&quot;:1670963430144}" id="Xar_GT8fRq3v" data-outputId="dc3db7c9-db92-4c97-cc9d-db5d20c569ba">
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Class example: &quot;</span>, classes[<span class="dv">10</span>])</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Class example:  Aston Martin Virage Coupe 2012
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="9" data-colab="{&quot;height&quot;:0,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:3240,&quot;timestamp&quot;:1670963433376}" id="LITwrjNtJfbf" data-outputId="40feedee-ac69-4449-ce89-17cad2731988">
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">16</span>):</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">4</span>, <span class="dv">4</span>, i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    plt.imshow(load_img(train_path <span class="op">+</span> train_dir[i]), cmap<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    plt.suptitle(<span class="st">&quot;Cars Training Sample&quot;</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">&#39;off&#39;</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/d66c257c0e9caf87cead3ca71037dff22bd8d501.png" /></p>
</div>
</div>
<section id="pre-processing" class="cell markdown" id="O1L3x9XGbc4A">
<h1><strong>Pre-Processing</strong></h1>
</section>
<section id="pre-processing-images" class="cell markdown" id="dfP_caos36EJ">
<h2>Pre Processing images</h2>
</section>
<div class="cell markdown" id="7idFX1sa3F6-">
<p>The data we got already defines the bounding box (bbox) of each of the images. Here we will extract the bbox definitions to crop the image later.</p>
</div>
<div class="cell code" data-execution_count="10" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:5,&quot;timestamp&quot;:1670963433376}" id="Vo3b2gR0voqI">
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_img_data(path):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    cars_annos <span class="op">=</span> sio.loadmat(path)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    annotations <span class="op">=</span> cars_annos[<span class="st">&#39;annotations&#39;</span>]</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    annotations <span class="op">=</span> np.transpose(annotations)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    fnames <span class="op">=</span> []</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    bboxes <span class="op">=</span> []</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> annotation <span class="kw">in</span> annotations:</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        bbox_x1 <span class="op">=</span> annotation[<span class="dv">0</span>][<span class="dv">0</span>][<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        bbox_y1 <span class="op">=</span> annotation[<span class="dv">0</span>][<span class="dv">1</span>][<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        bbox_x2 <span class="op">=</span> annotation[<span class="dv">0</span>][<span class="dv">2</span>][<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        bbox_y2 <span class="op">=</span> annotation[<span class="dv">0</span>][<span class="dv">3</span>][<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>        fname <span class="op">=</span> annotation[<span class="dv">0</span>][<span class="dv">4</span>][<span class="dv">0</span>]</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>        bboxes.append((bbox_x1, bbox_y1, bbox_x2, bbox_y2))</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>        fnames.append(fname)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fnames, bboxes</span></code></pre></div>
</div>
<section id="resize-image-function" class="cell markdown" id="0ChL5FLS4JT3">
<h3>Resize image function</h3>
<p>As has been mentioned before, our images have different sizes and dimensions. In order to work with them we will have to resize them all to a common size. Using the bboxes mentioned earlier we will also crop the images to reduce the noise while still leaving a small margin.</p>
</section>
<div class="cell code" data-execution_count="11" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:6,&quot;timestamp&quot;:1670963433377}" id="730PqINt4mkj">
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>picture_dim <span class="op">=</span> (<span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> resize_images(path, annos, path_to_save, crop<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    image_names <span class="op">=</span> <span class="bu">sorted</span>(os.listdir(path))</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> crop:</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>      _, bboxes <span class="op">=</span> process_img_data(annos)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(image_names)):      </span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>      image_name <span class="op">=</span> image_names[idx]</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>      im <span class="op">=</span> cv2.imread(path <span class="op">+</span> <span class="st">&quot;/&quot;</span> <span class="op">+</span> image_name)[:,:,::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> crop:</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>        (x1, y1, x2, y2) <span class="op">=</span> bboxes[idx]</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>        height, width <span class="op">=</span> im.shape[:<span class="dv">2</span>]</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># margins of 16 pixels</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>        margin <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>        x1 <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, x1 <span class="op">-</span> margin)</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>        y1 <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, y1 <span class="op">-</span> margin)</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>        x2 <span class="op">=</span> <span class="bu">min</span>(x2 <span class="op">+</span> margin, width)</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>        y2 <span class="op">=</span> <span class="bu">min</span>(y2 <span class="op">+</span> margin, height)</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>        im <span class="op">=</span> im[y1:y2, x1:x2]</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>      channels, h_resize ,w_resize <span class="op">=</span> picture_dim   </span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>      im <span class="op">=</span> cv2.resize(im, (h_resize,w_resize), interpolation<span class="op">=</span>cv2.INTER_LINEAR)</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>      im <span class="op">=</span> im.astype(np.float32)</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>      cv2.imwrite(path_to_save <span class="op">+</span> <span class="st">&quot;/&quot;</span> <span class="op">+</span> image_name, im)</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>  </span></code></pre></div>
</div>
<div class="cell code" data-execution_count="12" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:36878,&quot;timestamp&quot;:1670963470249}" id="hiMb7Eg6-1Ry">
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">sum</span>(<span class="bu">len</span>(files) <span class="cf">for</span> _, _, files <span class="kw">in</span> os.walk(cropped_train_path)) <span class="op">&lt;</span> <span class="bu">sum</span>(<span class="bu">len</span>(files) <span class="cf">for</span> _, _, files <span class="kw">in</span> os.walk(train_path)):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    resize_images(train_path, train_annos_path, cropped_train_path)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">sum</span>(<span class="bu">len</span>(files) <span class="cf">for</span> _, _, files <span class="kw">in</span> os.walk(cropped_test_path)) <span class="op">&lt;</span> <span class="bu">sum</span>(<span class="bu">len</span>(files) <span class="cf">for</span> _, _, files <span class="kw">in</span> os.walk(test_path)):    </span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    resize_images(test_path, test_annos_path, cropped_test_path)</span></code></pre></div>
</div>
<div class="cell markdown" id="Zw_Q8xxtA_hW">
<p>Here we can see the new image size:</p>
</div>
<div class="cell code" data-execution_count="13" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:19,&quot;timestamp&quot;:1670963470250}" id="Klrn2Xff_7fy" data-outputId="1445e633-87ac-4fc0-e24b-2812ebdffb0b">
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>image_names <span class="op">=</span> <span class="bu">sorted</span>(os.listdir(cropped_train_path))</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>image_name <span class="op">=</span> image_names[<span class="dv">0</span>]</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> cv2.imread(cropped_train_path <span class="op">+</span> <span class="st">&quot;/&quot;</span> <span class="op">+</span> image_name)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>im.shape</span></code></pre></div>
<div class="output execute_result" data-execution_count="13">
<pre><code>(224, 224, 3)</code></pre>
</div>
</div>
<section id="creating-labels" class="cell markdown" id="JoT3rZEEDjtW">
<h3>Creating labels</h3>
</section>
<div class="cell code" data-execution_count="14" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:18,&quot;timestamp&quot;:1670963470251}" id="p0niofNpDsqa">
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_img_class(path):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    cars_annos <span class="op">=</span> sio.loadmat(path)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    annotations <span class="op">=</span> cars_annos[<span class="st">&#39;annotations&#39;</span>][<span class="dv">0</span>]</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    classes <span class="op">=</span> []</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> annotation <span class="kw">in</span> annotations:</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        tag <span class="op">=</span> annotation[<span class="dv">4</span>][<span class="dv">0</span>][<span class="dv">0</span>]<span class="op">-</span><span class="dv">1</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        classes.append(tag)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> classes</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="15" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:18,&quot;timestamp&quot;:1670963470251}" id="5mHcFx04DZq7">
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>y_train_labels <span class="op">=</span> process_img_class(train_annos_path)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>y_test_labels <span class="op">=</span> process_img_class(test_annos_path)</span></code></pre></div>
</div>
<div class="cell markdown" id="uyKYqbzsVAn5">
<p>Writing labels to a csv file</p>
</div>
<div class="cell code" data-execution_count="16" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:17,&quot;timestamp&quot;:1670963470251}" id="bdPSERJzUqz2">
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>df_train <span class="op">=</span> pd.DataFrame(y_train_labels)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>df_train.to_csv(path_or_buf<span class="op">=</span>y_train_labels_path, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>df_test <span class="op">=</span> pd.DataFrame(y_test_labels)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>df_test.to_csv(path_or_buf<span class="op">=</span>y_test_labels_path, index<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="17" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:18,&quot;timestamp&quot;:1670963470252}" id="ZWAbpuifUbdt">
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> read_label(path,idx):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>  label <span class="op">=</span> pd.read_csv(path)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>  label <span class="op">=</span> label.iloc[idx, <span class="dv">0</span>]</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> label</span></code></pre></div>
</div>
<section id="train-data-balance" class="cell markdown" id="Er-1Hlok5BBe">
<h2>Train Data Balance</h2>
</section>
<div class="cell markdown" id="6zSZPLpJZCYw">
<p>Now that we have our ground truth established we can test whether the data is balanced:</p>
</div>
<div class="cell code" data-execution_count="18" data-colab="{&quot;height&quot;:0,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:591,&quot;timestamp&quot;:1670963470826}" id="fmG5dfIfZTj6" data-outputId="53c5e4c9-8bb6-4a3d-fb3d-a968a6cbd137">
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>sorted_classes <span class="op">=</span> <span class="bu">sorted</span>(y_train_labels)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> <span class="bu">list</span>(Counter(sorted_classes).keys())</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>y_values <span class="op">=</span> <span class="bu">list</span>(Counter(sorted_classes).values())</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>[<span class="dv">30</span>,<span class="dv">5</span>])</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>plt.bar(x_values, y_values, figure<span class="op">=</span>fig, align<span class="op">=</span><span class="st">&#39;edge&#39;</span>, color<span class="op">=</span>[<span class="st">&#39;red&#39;</span>, <span class="st">&#39;green&#39;</span>])</span></code></pre></div>
<div class="output execute_result" data-execution_count="18">
<pre><code>&lt;BarContainer object of 196 artists&gt;</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/b76d6228d41eba90223f7da07daafac372b718ab.png" /></p>
</div>
</div>
<section id="test-data-balance" class="cell markdown" id="HiZLbQJu42Ga">
<h2>Test Data Balance</h2>
</section>
<div class="cell code" data-execution_count="19" data-colab="{&quot;height&quot;:0,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:466,&quot;timestamp&quot;:1670963471289}" id="WTdPDjR641n9" data-outputId="4cc573e8-40af-4367-c9d4-7981b0d4fc69">
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>sorted_classes <span class="op">=</span> <span class="bu">sorted</span>(y_test_labels)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> <span class="bu">list</span>(Counter(sorted_classes).keys())</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>y_values <span class="op">=</span> <span class="bu">list</span>(Counter(sorted_classes).values())</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>[<span class="dv">30</span>,<span class="dv">5</span>])</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>plt.bar(x_values, y_values, figure<span class="op">=</span>fig, align<span class="op">=</span><span class="st">&#39;edge&#39;</span>, color<span class="op">=</span>[<span class="st">&#39;red&#39;</span>, <span class="st">&#39;green&#39;</span>])</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<div class="output execute_result" data-execution_count="19">
<pre><code>&lt;BarContainer object of 196 artists&gt;</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/a261f09296ae4a5e3e1921ee992da44f2f2e2502.png" /></p>
</div>
</div>
<div class="cell markdown" id="CQd8jZb3bFsO">
<p>From the results we can establish that each class has a balanced representation in the dataset samples, which will assist our model's convergence.</p>
</div>
<section id="benchmark-results" class="cell markdown" id="KRVsMlieotxe">
<h2>Benchmark results</h2>
</section>
<div class="cell markdown" id="jnX3Q7uzox1q">
<p>There are various benchmarks for this dataset, some of them include:</p>
<p><a href="https://www.kaggle.com/code/meaninglesslives/cars-se-resnext50-fastai" class="uri">https://www.kaggle.com/code/meaninglesslives/cars-se-resnext50-fastai</a> That displays a 92% test accuracy and utilizes ResNeXt pretrained models.</p>
<p><a href="https://www.kaggle.com/code/meaninglesslives/cars-eb3-keras" class="uri">https://www.kaggle.com/code/meaninglesslives/cars-eb3-keras</a> Reports a 90% test accuracy and is based upon EfficientNet pretrained on ImageNet.</p>
<p><a href="https://www.kaggle.com/code/meaninglesslives/cars-eb0-keras" class="uri">https://www.kaggle.com/code/meaninglesslives/cars-eb0-keras</a> Reports a 90% test accuracy and is based upon EfficientNetB0 pretrained on ImageNet.</p>
<p><a href="https://www.kaggle.com/code/abdelrahmant11/car-type-classification" class="uri">https://www.kaggle.com/code/abdelrahmant11/car-type-classification</a> Peaks with 21% validation accuracy using a model with 3 convolution layers containing 1 convolution+max pooling each and a dense+dropout layer.</p>
</div>
<section id="create-the-dataset-object" class="cell markdown" id="2vT5f-Ux2PZG">
<h2>Create the dataset object</h2>
</section>
<div class="cell code" data-execution_count="20" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:5,&quot;timestamp&quot;:1670963471289}" id="L4gCEAjT2Ovw">
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset,DataLoader</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.io <span class="im">import</span> read_image</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> cars_dataset(Dataset):</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, path_to_imgs, labels_path, transform<span class="op">=</span><span class="va">None</span>, target_transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">## assumptions:</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">## labels_file contains filename and label for that file</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels_path <span class="op">=</span> labels_path</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.path_to_imgs <span class="op">=</span> path_to_imgs</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_transform <span class="op">=</span> target_transform</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> pd.read_csv(<span class="va">self</span>.labels_path)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> data.size</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>,idx):</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>        image_name <span class="op">=</span> <span class="bu">sorted</span>(os.listdir(<span class="va">self</span>.path_to_imgs))[idx]</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>        im <span class="op">=</span> cv2.imread(<span class="va">self</span>.path_to_imgs <span class="op">+</span> <span class="st">&quot;/&quot;</span> <span class="op">+</span> image_name)[:,:,::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># channels need to be first (channel x height x width)</span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>        im <span class="op">=</span> im.transpose()</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> read_label(<span class="va">self</span>.labels_path, idx)</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transform:</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>            im <span class="op">=</span> <span class="va">self</span>.transform(im)</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.target_transform:</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>            label <span class="op">=</span> <span class="va">self</span>.target_transform(label)</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> im.astype(np.float32).copy(), label</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="21" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:5,&quot;timestamp&quot;:1670963471290}" id="m1Ta4bXiDHxu" data-outputId="c034ba57-29ab-4b6f-b882-01f12e2c455d">
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>cropped_cars_train_dataset <span class="op">=</span> cars_dataset(cropped_train_path, y_train_labels_path)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>cropped_cars_test_dataset <span class="op">=</span> cars_dataset(cropped_test_path, y_test_labels_path)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>im, label <span class="op">=</span> cropped_cars_train_dataset.<span class="fu">__getitem__</span>(<span class="dv">0</span>)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;image shape:</span><span class="sc">{shape}</span><span class="st"> label: </span><span class="sc">{label}</span><span class="st">&quot;</span>.<span class="bu">format</span>(shape <span class="op">=</span> im.shape, label <span class="op">=</span> label))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>image shape:(3, 224, 224) label: 13
</code></pre>
</div>
</div>
<section id="create-kfolds" class="cell markdown" id="uUtKAcHN5R_T">
<h2>Create KFolds</h2>
</section>
<div class="cell code" data-execution_count="22" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:5,&quot;timestamp&quot;:1670963471290}" id="dKqpvDr5FSPx">
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>k_folds <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>kfold <span class="op">=</span> KFold(n_splits<span class="op">=</span>k_folds, shuffle<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<section id="forming-the-neural-network" class="cell markdown" id="AiGH0tq53sQz">
<h1><strong>Forming the neural network</strong></h1>
</section>
<section id="preparing-common-functions-and-variables" class="cell markdown" id="FTd6ofqnIV8X">
<h2>Preparing common functions and variables</h2>
</section>
<div class="cell code" data-execution_count="23" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:5,&quot;timestamp&quot;:1670963471749}" id="CPwIYngMSii0">
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">8</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="24" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:5,&quot;timestamp&quot;:1670963471749}" id="CGbVjO8Uy0TX">
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_loop(data_loader, model, device, loss_fn, optimizer, print_every_n<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(data_loader.dataset)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    train_loss<span class="op">=</span><span class="dv">0</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    true_positives<span class="op">=</span><span class="dv">0</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, (X,y) <span class="kw">in</span> <span class="bu">enumerate</span>(data_loader):</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> X.to(device)</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.to(dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.to(device)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">+=</span> loss</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>        true_positives <span class="op">+=</span> (y<span class="op">==</span>pred.argmax(<span class="dv">1</span>)).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch<span class="op">%</span>print_every_n<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>            loss, current <span class="op">=</span> loss.item(), batch<span class="op">*</span><span class="bu">len</span>(X)</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&#39;loss=</span><span class="sc">{</span>loss<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>current<span class="sc">}</span><span class="ss"> samples / </span><span class="sc">{</span>size<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">/=</span> num_batches</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>    train_acc <span class="op">=</span> true_positives<span class="op">/</span>size    </span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&#39;train accuracy = </span><span class="sc">{</span>train_acc<span class="sc">:.8f}</span><span class="ss">&#39;</span>)</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loss, train_acc</span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_loop(data_loader, model, device, loss_fn):</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span><span class="bu">len</span>(data_loader.dataset)</span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>    test_loss<span class="op">=</span><span class="dv">0</span></span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>    true_positives<span class="op">=</span><span class="dv">0</span></span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X,y <span class="kw">in</span> data_loader:</span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>            X <span class="op">=</span> X.to(device)</span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y.to(dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y.to(device)</span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> model(X)</span>
<span id="cb45-40"><a href="#cb45-40" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss_fn(pred,y).item()</span>
<span id="cb45-41"><a href="#cb45-41" aria-hidden="true" tabindex="-1"></a>            true_positives <span class="op">+=</span> (y<span class="op">==</span>pred.argmax(<span class="dv">1</span>)).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb45-42"><a href="#cb45-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-43"><a href="#cb45-43" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">/=</span> num_batches</span>
<span id="cb45-44"><a href="#cb45-44" aria-hidden="true" tabindex="-1"></a>    test_acc <span class="op">=</span> true_positives<span class="op">/</span>size</span>
<span id="cb45-45"><a href="#cb45-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&#39;test accuracy = </span><span class="sc">{</span>test_acc<span class="sc">}</span><span class="ss">, test loss = </span><span class="sc">{</span>test_loss<span class="sc">:2f}</span><span class="ss">&#39;</span>)</span>
<span id="cb45-46"><a href="#cb45-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> test_loss, test_acc</span></code></pre></div>
</div>
<section id="setting-model-train-function" class="cell markdown" id="kdPwv00wMs_g">
<h2>Setting Model Train Function</h2>
</section>
<div class="cell code" data-execution_count="25" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:5,&quot;timestamp&quot;:1670963471750}" id="82nV0XrBG70O">
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_and_validate_kfold(model, dataset, epochs, batch_size, learning_rate, loss_fn): </span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>  model_history <span class="op">=</span> []</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> fold, (train_idxs, val_idxs) <span class="kw">in</span> <span class="bu">enumerate</span>(kfold.split(dataset)):</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;fold number: &quot;</span> <span class="op">+</span> <span class="bu">str</span>(fold<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    train_subsampler <span class="op">=</span> torch.utils.data.SubsetRandomSampler(train_idxs)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>    val_subsampler <span class="op">=</span> torch.utils.data.SubsetRandomSampler(val_idxs)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>    current_history <span class="op">=</span> []</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span>batch_size, sampler<span class="op">=</span>train_subsampler)</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span>batch_size, sampler<span class="op">=</span>val_subsampler)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(<span class="st">&quot;epoch: &quot;</span>, epoch<span class="op">+</span><span class="dv">1</span>, <span class="st">&quot;/&quot;</span>, epochs)</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>      train_loss, train_acc <span class="op">=</span> train_loop(train_loader, model, device, loss_fn, optimizer)</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>      test_loss, test_acc <span class="op">=</span> test_loop(val_loader, model, device, loss_fn)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>      current_history.append({<span class="st">&#39;train_loss&#39;</span>:train_loss.detach().cpu().numpy(), <span class="st">&#39;val_loss&#39;</span>:test_loss, <span class="st">&#39;train_acc&#39;</span>:train_acc, <span class="st">&#39;val_acc&#39;</span>:test_acc})</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>()</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>    model_history.append(current_history)</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reset fold model weights after acquiring all relevant metrics</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> model.children():</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(layer, <span class="st">&#39;reset_parameters&#39;</span>):</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>           layer.reset_parameters()</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> model_history</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="26" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:5,&quot;timestamp&quot;:1670963471750}" id="tFVZZq2tYaN_">
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_mean_history(model_history, fold_count):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>  history <span class="op">=</span> [{<span class="st">&#39;train_loss&#39;</span>:<span class="dv">0</span>, <span class="st">&#39;val_loss&#39;</span>:<span class="dv">0</span>, <span class="st">&#39;train_acc&#39;</span>:<span class="dv">0</span>, <span class="st">&#39;val_acc&#39;</span>:<span class="dv">0</span>} <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs)]</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> fold <span class="kw">in</span> <span class="bu">range</span>(fold_count):</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>      history[epoch][<span class="st">&#39;train_loss&#39;</span>] <span class="op">+=</span> model_history[fold][epoch][<span class="st">&#39;train_loss&#39;</span>]</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>      history[epoch][<span class="st">&#39;val_loss&#39;</span>] <span class="op">+=</span> model_history[fold][epoch][<span class="st">&#39;val_loss&#39;</span>]</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>      history[epoch][<span class="st">&#39;train_acc&#39;</span>] <span class="op">+=</span> model_history[fold][epoch][<span class="st">&#39;train_acc&#39;</span>]</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>      history[epoch][<span class="st">&#39;val_acc&#39;</span>] <span class="op">+=</span> model_history[fold][epoch][<span class="st">&#39;val_acc&#39;</span>]</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>      history[epoch][<span class="st">&#39;train_loss&#39;</span>] <span class="op">/=</span> fold_count</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>      history[epoch][<span class="st">&#39;val_loss&#39;</span>] <span class="op">/=</span> fold_count</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>      history[epoch][<span class="st">&#39;train_acc&#39;</span>] <span class="op">/=</span> fold_count  </span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>      history[epoch][<span class="st">&#39;val_acc&#39;</span>] <span class="op">/=</span> fold_count</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> history</span></code></pre></div>
</div>
<section id="setting-plot-functions" class="cell markdown" id="IwaNwXEZ-zgU">
<h3>Setting plot functions</h3>
</section>
<div class="cell code" data-execution_count="27" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:5,&quot;timestamp&quot;:1670963471750}" id="5WJ9hQnc-yEe">
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_accuracy(epochs, history):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>  x_axis <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(epochs))</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>  acc_res <span class="op">=</span> [history[i][<span class="st">&#39;train_acc&#39;</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs)]</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>  test_res <span class="op">=</span> [history[i][<span class="st">&#39;val_acc&#39;</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs)]</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>  plt.plot(x_axis, acc_res, label <span class="op">=</span> <span class="st">&quot;train accuracy&quot;</span>)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>  plt.plot(x_axis, test_res, label <span class="op">=</span> <span class="st">&quot;validation accuracy&quot;</span>)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># naming the x axis</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>  plt.xlabel(<span class="st">&#39;epochs&#39;</span>)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># naming the y axis</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>  plt.ylabel(<span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># show a legend on the plot</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>  plt.legend()</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># function to show the plot</span></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>  plt.show()</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="67" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:5,&quot;timestamp&quot;:1670963471751}" id="pcJ5kGO4_ApU">
<div class="sourceCode" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_loss(epochs, history):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>  x_axis <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(epochs))</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>  train_loss_res <span class="op">=</span> [history[i][<span class="st">&#39;train_loss&#39;</span>].item(<span class="dv">0</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs)]</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>  val_loss_res <span class="op">=</span> [history[i][<span class="st">&#39;val_loss&#39;</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs)]</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>  plt.plot(x_axis, train_loss_res, label <span class="op">=</span> <span class="st">&quot;train loss&quot;</span>)</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>  plt.plot(x_axis, val_loss_res, label <span class="op">=</span> <span class="st">&quot;validation loss&quot;</span>)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># naming the x axis</span></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>  plt.xlabel(<span class="st">&#39;epochs&#39;</span>)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># naming the y axis</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>  plt.ylabel(<span class="st">&#39;loss&#39;</span>)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># show a legend on the plot</span></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>  plt.legend()</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># function to show the plot</span></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>  plt.show()</span></code></pre></div>
</div>
<section id="first-try" class="cell markdown" id="r-GSHCTcH-Pt">
<h2>First Try</h2>
</section>
<section id="defining-the-architecture" class="cell markdown" id="9tukNlbkPV0B">
<h3>Defining the architecture</h3>
</section>
<div class="cell markdown" id="Au5mv_a79PuI">
<p>We start out with a basic architecture structure:</p>
</div>
<div class="cell code" data-execution_count="29" id="TCkw8U4aR2Lm">
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNetV1(nn.Module):</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, in_height, in_width, max_pool_count):</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvNetV1, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>        out_width <span class="op">=</span> <span class="bu">int</span>(in_width <span class="op">/</span> (<span class="dv">2</span><span class="op">**</span>max_pool_count))</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>        out_height <span class="op">=</span> <span class="bu">int</span>(in_height <span class="op">/</span> (<span class="dv">2</span><span class="op">**</span>max_pool_count))</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>        out_channels <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flattened_dimensions <span class="op">=</span> out_channels <span class="op">*</span> out_width <span class="op">*</span> out_height</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_channels, <span class="dv">64</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># max pooling layer</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="va">self</span>.flattened_dimensions, <span class="dv">500</span>)</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">500</span>, <span class="dv">196</span>)</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.25</span>)</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):        </span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add sequence of convolutional and max pooling layers</span></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv1(x)) <span class="co"># 3-&gt;64</span></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv2(x)) <span class="co"># 64-&gt;64</span></span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv3(x)) <span class="co"># 64-&gt;64</span></span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># max pooling</span></span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>       </span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># flatten image input</span></span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.flattened_dimensions)</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add dropout layer</span></span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># fully connected</span></span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>channels, height, width <span class="op">=</span> picture_dim</span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ConvNetV1(channels, height, width, <span class="dv">1</span>).to(device)</span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span></code></pre></div>
</div>
<section id="results" class="cell markdown" id="oFtHfF7WI34c">
<h3>Results</h3>
</section>
<div class="cell code" data-execution_count="30" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;error&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:840797,&quot;timestamp&quot;:1670697255632}" id="QKxw3eM-MVoi" data-outputId="fb9ce2c4-c0be-4d31-8958-664967d1061f">
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>model_history <span class="op">=</span> train_and_validate_kfold(model, cropped_cars_train_dataset, epochs<span class="op">=</span>epochs, batch_size<span class="op">=</span>batch_size, learning_rate<span class="op">=</span>learning_rate, loss_fn<span class="op">=</span>loss_fn)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>fold number: 1
epoch:  1 / 5
loss=7.003, 0 samples / 8144
loss=5.189, 800 samples / 8144
loss=5.292, 1600 samples / 8144
loss=5.345, 2400 samples / 8144
loss=5.286, 3200 samples / 8144
loss=5.264, 4000 samples / 8144
loss=5.324, 4800 samples / 8144
loss=5.255, 5600 samples / 8144
loss=5.303, 6400 samples / 8144
train accuracy = 0.00540275
test accuracy = 0.0018418467583497054, test loss = 5.247810

epoch:  2 / 5
loss=4.643, 0 samples / 8144
loss=4.091, 800 samples / 8144
loss=4.377, 1600 samples / 8144
loss=2.860, 2400 samples / 8144
loss=3.209, 3200 samples / 8144
loss=3.604, 4000 samples / 8144
loss=3.280, 4800 samples / 8144
loss=4.338, 5600 samples / 8144
loss=3.615, 6400 samples / 8144
train accuracy = 0.15839882
test accuracy = 0.007490176817288801, test loss = 5.373935

epoch:  3 / 5
loss=1.321, 0 samples / 8144
loss=0.825, 800 samples / 8144
loss=0.333, 1600 samples / 8144
loss=0.556, 2400 samples / 8144
loss=0.034, 3200 samples / 8144
loss=0.001, 4000 samples / 8144
loss=0.000, 4800 samples / 8144
loss=0.374, 5600 samples / 8144
loss=0.002, 6400 samples / 8144
train accuracy = 0.75478880
test accuracy = 0.007244597249508841, test loss = 10.153093

epoch:  4 / 5
loss=0.037, 0 samples / 8144
loss=0.262, 800 samples / 8144
loss=0.009, 1600 samples / 8144
loss=0.000, 2400 samples / 8144
loss=0.001, 3200 samples / 8144
loss=0.046, 4000 samples / 8144
loss=0.034, 4800 samples / 8144
loss=0.003, 5600 samples / 8144
loss=0.006, 6400 samples / 8144
train accuracy = 0.79666012
test accuracy = 0.0068762278978389, test loss = 7.052854

epoch:  5 / 5
loss=0.009, 0 samples / 8144
loss=0.002, 800 samples / 8144
loss=0.000, 1600 samples / 8144
loss=0.000, 2400 samples / 8144
loss=0.002, 3200 samples / 8144
loss=0.003, 4000 samples / 8144
loss=0.002, 4800 samples / 8144
loss=0.006, 5600 samples / 8144
loss=0.000, 6400 samples / 8144
train accuracy = 0.79715128
test accuracy = 0.0054027504911591355, test loss = 8.484081

fold number: 2
epoch:  1 / 5
loss=8.538, 0 samples / 8144
loss=5.280, 800 samples / 8144
loss=5.256, 1600 samples / 8144
loss=5.232, 2400 samples / 8144
loss=5.249, 3200 samples / 8144
loss=5.047, 4000 samples / 8144
loss=5.185, 4800 samples / 8144
loss=5.265, 5600 samples / 8144
loss=5.058, 6400 samples / 8144
train accuracy = 0.00663065
test accuracy = 0.002087426326129666, test loss = 5.248467

epoch:  2 / 5
loss=4.766, 0 samples / 8144
loss=2.997, 800 samples / 8144
loss=3.207, 1600 samples / 8144
loss=3.217, 2400 samples / 8144
loss=3.836, 3200 samples / 8144
loss=4.236, 4000 samples / 8144
loss=4.900, 4800 samples / 8144
loss=3.064, 5600 samples / 8144
loss=3.565, 6400 samples / 8144
train accuracy = 0.18025540
test accuracy = 0.007858546168958742, test loss = 5.487469

epoch:  3 / 5
loss=0.001, 0 samples / 8144
loss=0.013, 800 samples / 8144
loss=0.001, 1600 samples / 8144
loss=0.319, 2400 samples / 8144
loss=0.388, 3200 samples / 8144
loss=0.000, 4000 samples / 8144
loss=0.086, 4800 samples / 8144
loss=0.107, 5600 samples / 8144
loss=0.306, 6400 samples / 8144
train accuracy = 0.77136542
test accuracy = 0.008840864440078585, test loss = 8.317758

epoch:  4 / 5
loss=0.001, 0 samples / 8144
loss=0.000, 800 samples / 8144
loss=0.000, 1600 samples / 8144
loss=0.314, 2400 samples / 8144
loss=0.013, 3200 samples / 8144
loss=0.000, 4000 samples / 8144
loss=0.002, 4800 samples / 8144
loss=0.002, 5600 samples / 8144
loss=0.013, 6400 samples / 8144
train accuracy = 0.79764244
test accuracy = 0.008595284872298626, test loss = 6.312062

epoch:  5 / 5
loss=0.000, 0 samples / 8144
loss=0.001, 800 samples / 8144
loss=0.000, 1600 samples / 8144
loss=0.028, 2400 samples / 8144
loss=0.003, 3200 samples / 8144
loss=0.000, 4000 samples / 8144
loss=0.041, 4800 samples / 8144
loss=0.056, 5600 samples / 8144
loss=0.000, 6400 samples / 8144
train accuracy = 0.79727407
test accuracy = 0.008840864440078585, test loss = 6.494044

fold number: 3
epoch:  1 / 5
loss=6.059, 0 samples / 8144
loss=5.309, 800 samples / 8144
loss=5.239, 1600 samples / 8144
loss=5.390, 2400 samples / 8144
loss=5.314, 3200 samples / 8144
loss=5.193, 4000 samples / 8144
loss=5.322, 4800 samples / 8144
loss=5.267, 5600 samples / 8144
loss=5.429, 6400 samples / 8144
train accuracy = 0.00736739
test accuracy = 0.0033153241650294694, test loss = 5.177624

epoch:  2 / 5
loss=4.596, 0 samples / 8144
loss=4.290, 800 samples / 8144
loss=3.621, 1600 samples / 8144
loss=2.522, 2400 samples / 8144
loss=3.360, 3200 samples / 8144
loss=2.349, 4000 samples / 8144
loss=4.580, 4800 samples / 8144
loss=2.728, 5600 samples / 8144
loss=2.317, 6400 samples / 8144
train accuracy = 0.26915521
test accuracy = 0.006753438113948919, test loss = 5.878904

epoch:  3 / 5
loss=0.238, 0 samples / 8144
loss=0.501, 800 samples / 8144
loss=0.727, 1600 samples / 8144
loss=0.057, 2400 samples / 8144
loss=0.470, 3200 samples / 8144
loss=0.000, 4000 samples / 8144
loss=0.018, 4800 samples / 8144
loss=0.000, 5600 samples / 8144
loss=0.000, 6400 samples / 8144
train accuracy = 0.76633104
test accuracy = 0.009332023575638507, test loss = 9.878877

epoch:  4 / 5
loss=0.000, 0 samples / 8144
loss=0.000, 800 samples / 8144
loss=0.074, 1600 samples / 8144
loss=0.000, 2400 samples / 8144
loss=0.011, 3200 samples / 8144
loss=0.003, 4000 samples / 8144
loss=0.005, 4800 samples / 8144
loss=0.000, 5600 samples / 8144
loss=0.009, 6400 samples / 8144
train accuracy = 0.79666012
test accuracy = 0.008840864440078585, test loss = 9.095886

epoch:  5 / 5
loss=0.012, 0 samples / 8144
loss=0.000, 800 samples / 8144
loss=0.003, 1600 samples / 8144
loss=0.001, 2400 samples / 8144
loss=0.000, 3200 samples / 8144
loss=0.005, 4000 samples / 8144
loss=0.000, 4800 samples / 8144
loss=0.000, 5600 samples / 8144
loss=0.000, 6400 samples / 8144
train accuracy = 0.79862475
test accuracy = 0.008226915520628684, test loss = 9.504078

fold number: 4
epoch:  1 / 5
loss=8.173, 0 samples / 8144
loss=5.339, 800 samples / 8144
loss=5.286, 1600 samples / 8144
loss=5.231, 2400 samples / 8144
loss=5.251, 3200 samples / 8144
loss=5.268, 4000 samples / 8144
loss=5.337, 4800 samples / 8144
loss=5.339, 5600 samples / 8144
loss=5.355, 6400 samples / 8144
train accuracy = 0.00712181
test accuracy = 0.001719056974459725, test loss = 5.245207

epoch:  2 / 5
loss=4.719, 0 samples / 8144
loss=1.708, 800 samples / 8144
loss=2.694, 1600 samples / 8144
loss=2.395, 2400 samples / 8144
loss=3.342, 3200 samples / 8144
loss=3.005, 4000 samples / 8144
loss=3.835, 4800 samples / 8144
loss=2.221, 5600 samples / 8144
loss=1.756, 6400 samples / 8144
train accuracy = 0.28278487
test accuracy = 0.006753438113948919, test loss = 5.722911

epoch:  3 / 5
loss=0.345, 0 samples / 8144
loss=0.000, 800 samples / 8144
loss=0.000, 1600 samples / 8144
loss=0.001, 2400 samples / 8144
loss=0.007, 3200 samples / 8144
loss=0.029, 4000 samples / 8144
loss=0.000, 4800 samples / 8144
loss=0.000, 5600 samples / 8144
loss=0.000, 6400 samples / 8144
train accuracy = 0.78462672
test accuracy = 0.0068762278978389, test loss = 8.831281

epoch:  4 / 5
loss=0.000, 0 samples / 8144
loss=0.000, 800 samples / 8144
loss=0.004, 1600 samples / 8144
loss=0.001, 2400 samples / 8144
loss=3.833, 3200 samples / 8144
loss=0.000, 4000 samples / 8144
loss=0.017, 4800 samples / 8144
loss=0.164, 5600 samples / 8144
loss=0.079, 6400 samples / 8144
train accuracy = 0.79764244
test accuracy = 0.0068762278978389, test loss = 6.317921

epoch:  5 / 5
loss=0.004, 0 samples / 8144
loss=0.000, 800 samples / 8144
loss=3.037, 1600 samples / 8144
loss=0.000, 2400 samples / 8144
loss=0.000, 3200 samples / 8144
loss=0.000, 4000 samples / 8144
loss=0.002, 4800 samples / 8144
loss=0.000, 5600 samples / 8144
loss=0.006, 6400 samples / 8144
train accuracy = 0.79862475
test accuracy = 0.0073673870333988214, test loss = 8.041570

fold number: 5
epoch:  1 / 5
loss=9.465, 0 samples / 8144
loss=5.302, 800 samples / 8144
loss=5.307, 1600 samples / 8144
loss=5.351, 2400 samples / 8144
loss=5.200, 3200 samples / 8144
loss=5.275, 4000 samples / 8144
loss=5.302, 4800 samples / 8144
loss=5.278, 5600 samples / 8144
loss=5.341, 6400 samples / 8144
train accuracy = 0.00429764
test accuracy = 0.0019646365422396855, test loss = 5.245061

epoch:  2 / 5
loss=4.322, 0 samples / 8144
loss=4.276, 800 samples / 8144
loss=4.025, 1600 samples / 8144
loss=4.067, 2400 samples / 8144
loss=4.430, 3200 samples / 8144
loss=4.442, 4000 samples / 8144
loss=4.292, 4800 samples / 8144
loss=4.608, 5600 samples / 8144
loss=2.654, 6400 samples / 8144
train accuracy = 0.16060904
test accuracy = 0.009086444007858546, test loss = 5.339799

epoch:  3 / 5
loss=1.469, 0 samples / 8144
loss=0.001, 800 samples / 8144
loss=0.064, 1600 samples / 8144
loss=0.731, 2400 samples / 8144
loss=1.120, 3200 samples / 8144
loss=0.470, 4000 samples / 8144
loss=1.010, 4800 samples / 8144
loss=0.125, 5600 samples / 8144
loss=0.000, 6400 samples / 8144
train accuracy = 0.75970039
test accuracy = 0.008472495088408645, test loss = 8.538005

epoch:  4 / 5
loss=0.014, 0 samples / 8144
loss=0.316, 800 samples / 8144
loss=0.168, 1600 samples / 8144
loss=0.000, 2400 samples / 8144
loss=0.000, 3200 samples / 8144
loss=0.007, 4000 samples / 8144
loss=0.001, 4800 samples / 8144
loss=0.001, 5600 samples / 8144
loss=0.001, 6400 samples / 8144
train accuracy = 0.79739686
test accuracy = 0.008472495088408645, test loss = 6.997886

epoch:  5 / 5
loss=0.001, 0 samples / 8144
loss=0.000, 800 samples / 8144
loss=0.000, 1600 samples / 8144
loss=0.001, 2400 samples / 8144
loss=0.007, 3200 samples / 8144
loss=4.394, 4000 samples / 8144
loss=0.003, 4800 samples / 8144
loss=0.000, 5600 samples / 8144
loss=0.008, 6400 samples / 8144
train accuracy = 0.79899312
test accuracy = 0.0073673870333988214, test loss = 6.429724

</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="52" id="M4cQWLRf_7o5">
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> get_mean_history(model_history, k_folds)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="54" id="zVMyNCFxI_K5">
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>plot_accuracy(epochs, history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/e012a72105419f8e1c52d95fef19608b7d38e40a.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="55" id="uTzH2aFtIEM7">
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>plot_loss(epochs, history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/440e113a1948513ab119ada2e95a86c46e3a4625.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="56" id="ZtT1S57YdO2i">
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>history_dataframe <span class="op">=</span> pd.DataFrame(history)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>history_dataframe</span></code></pre></div>
<div class="output execute_result" data-execution_count="56">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train_loss</th>
      <th>val_loss</th>
      <th>train_acc</th>
      <th>val_acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8.448141</td>
      <td>5.232834</td>
      <td>0.006164</td>
      <td>0.002186</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.768009</td>
      <td>5.560604</td>
      <td>0.210241</td>
      <td>0.007588</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.281176</td>
      <td>9.143803</td>
      <td>0.767362</td>
      <td>0.008153</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.080759</td>
      <td>7.155322</td>
      <td>0.797200</td>
      <td>0.007932</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.051019</td>
      <td>7.790699</td>
      <td>0.798134</td>
      <td>0.007441</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell markdown" id="_qPkNGqB_LgQ">
<p>From the results we gather that our model is overfitting and not generalizing the input enough but rather memorizing it.</p>
</div>
<div class="cell markdown" id="dIQRevNb_Wm2">
<p>Since our model is overfitting we can try:</p>
<ol>
<li><p>Adding another dropout layer and convolution layers with less convolutions per layer to increase the amount of channels the model uses to analyze the input.</p></li>
<li><p>Adding augmented samples to make it harder for our model to "just" memorize all the samples and to try and "force" it to generalize the input data more.</p></li>
<li><p>Adding an additional fully connected layer with more output channels to ensure our output is more specific to the input features before moving them on to the final prediction fully connected layer.</p></li>
</ol>
</div>
<section id="second-try" class="cell markdown" id="9l9KuIAWJWMu">
<h2>Second Try</h2>
</section>
<section id="defining-the-architecture" class="cell markdown" id="N6t-hhRmPcVp">
<h3>Defining the architecture</h3>
</section>
<div class="cell code" data-execution_count="29" id="WHeQX18QjXJw">
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNetV2(nn.Module):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, in_height, in_width, max_pool_count):</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvNetV2, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>        out_height <span class="op">=</span> <span class="bu">int</span>(in_height <span class="op">/</span> (<span class="dv">2</span><span class="op">**</span>max_pool_count))</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>        out_width <span class="op">=</span> <span class="bu">int</span>(in_width <span class="op">/</span> (<span class="dv">2</span><span class="op">**</span>max_pool_count))</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>        out_channels <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flattened_dimensions <span class="op">=</span> out_channels <span class="op">*</span> out_height <span class="op">*</span> out_width</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># layers</span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_channels, <span class="dv">64</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv21 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv22 <span class="op">=</span> nn.Conv2d(<span class="dv">128</span>, <span class="dv">128</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv31 <span class="op">=</span> nn.Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv32 <span class="op">=</span> nn.Conv2d(<span class="dv">256</span>, <span class="dv">256</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="va">self</span>.flattened_dimensions, <span class="dv">2000</span>)</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">2000</span>, <span class="dv">1000</span>)</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">1000</span>, <span class="dv">196</span>)</span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.25</span>)</span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1st convolution</span></span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv1(x)) <span class="co"># 3-&gt;64</span></span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv2(x)) <span class="co"># 64-&gt;64</span></span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># max pool</span></span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-35"><a href="#cb57-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2nd convolution</span></span>
<span id="cb57-36"><a href="#cb57-36" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv21(x)) <span class="co"># 64-&gt;128</span></span>
<span id="cb57-37"><a href="#cb57-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv22(x)) <span class="co"># 128-&gt;128</span></span>
<span id="cb57-38"><a href="#cb57-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-39"><a href="#cb57-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># max pool</span></span>
<span id="cb57-40"><a href="#cb57-40" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb57-41"><a href="#cb57-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-42"><a href="#cb57-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3rd convolution</span></span>
<span id="cb57-43"><a href="#cb57-43" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv31(x)) <span class="co"># 128-&gt;256</span></span>
<span id="cb57-44"><a href="#cb57-44" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv32(x)) <span class="co"># 256-&gt;256</span></span>
<span id="cb57-45"><a href="#cb57-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-46"><a href="#cb57-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># max pool</span></span>
<span id="cb57-47"><a href="#cb57-47" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb57-48"><a href="#cb57-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-49"><a href="#cb57-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># flatten image input</span></span>
<span id="cb57-50"><a href="#cb57-50" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.flattened_dimensions)</span>
<span id="cb57-51"><a href="#cb57-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-52"><a href="#cb57-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dropout</span></span>
<span id="cb57-53"><a href="#cb57-53" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb57-54"><a href="#cb57-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1st fully connected</span></span>
<span id="cb57-55"><a href="#cb57-55" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb57-56"><a href="#cb57-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dropout</span></span>
<span id="cb57-57"><a href="#cb57-57" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb57-58"><a href="#cb57-58" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.fc3(x))</span>
<span id="cb57-59"><a href="#cb57-59" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb57-60"><a href="#cb57-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2nd fully connected</span></span>
<span id="cb57-61"><a href="#cb57-61" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb57-62"><a href="#cb57-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-63"><a href="#cb57-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb57-64"><a href="#cb57-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-65"><a href="#cb57-65" aria-hidden="true" tabindex="-1"></a>channels, height, width <span class="op">=</span> picture_dim</span>
<span id="cb57-66"><a href="#cb57-66" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ConvNetV2(channels, height, width, <span class="dv">3</span>).to(device)</span>
<span id="cb57-67"><a href="#cb57-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-68"><a href="#cb57-68" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">5e-6</span></span>
<span id="cb57-69"><a href="#cb57-69" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb57-70"><a href="#cb57-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-71"><a href="#cb57-71" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">8</span></span></code></pre></div>
</div>
<section id="results" class="cell markdown" id="74DrGlM3DWn1">
<h3>Results</h3>
</section>
<div class="cell code" data-execution_count="30" id="6hL4bIStQKYv">
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>model_history <span class="op">=</span> train_and_validate_kfold(model, cropped_cars_train_dataset, epochs<span class="op">=</span>epochs, batch_size<span class="op">=</span>batch_size, learning_rate<span class="op">=</span>learning_rate, loss_fn<span class="op">=</span>loss_fn)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>fold number: 1
epoch:  1 / 8
loss=5.249, 0 samples / 8144
loss=5.350, 800 samples / 8144
loss=5.284, 1600 samples / 8144
loss=5.353, 2400 samples / 8144
loss=5.270, 3200 samples / 8144
loss=5.292, 4000 samples / 8144
loss=5.268, 4800 samples / 8144
loss=5.269, 5600 samples / 8144
loss=5.295, 6400 samples / 8144
train accuracy = 0.00380648
test accuracy = 0.0013506876227897839, test loss = 5.246530

epoch:  2 / 8
loss=5.246, 0 samples / 8144
loss=5.008, 800 samples / 8144
loss=5.304, 1600 samples / 8144
loss=5.399, 2400 samples / 8144
loss=5.099, 3200 samples / 8144
loss=5.189, 4000 samples / 8144
loss=5.226, 4800 samples / 8144
loss=5.097, 5600 samples / 8144
loss=5.097, 6400 samples / 8144
train accuracy = 0.01191061
test accuracy = 0.003806483300589391, test loss = 5.149503

epoch:  3 / 8
loss=5.206, 0 samples / 8144
loss=5.286, 800 samples / 8144
loss=5.106, 1600 samples / 8144
loss=4.146, 2400 samples / 8144
loss=5.157, 3200 samples / 8144
loss=5.233, 4000 samples / 8144
loss=4.991, 4800 samples / 8144
loss=4.937, 5600 samples / 8144
loss=4.186, 6400 samples / 8144
train accuracy = 0.02615422
test accuracy = 0.007612966601178782, test loss = 4.998248

epoch:  4 / 8
loss=4.253, 0 samples / 8144
loss=4.171, 800 samples / 8144
loss=4.835, 1600 samples / 8144
loss=4.783, 2400 samples / 8144
loss=4.424, 3200 samples / 8144
loss=4.566, 4000 samples / 8144
loss=4.226, 4800 samples / 8144
loss=4.370, 5600 samples / 8144
loss=4.288, 6400 samples / 8144
train accuracy = 0.07404224
test accuracy = 0.013015717092337918, test loss = 4.764639

epoch:  5 / 8
loss=4.144, 0 samples / 8144
loss=4.790, 800 samples / 8144
loss=3.977, 1600 samples / 8144
loss=3.363, 2400 samples / 8144
loss=2.345, 3200 samples / 8144
loss=3.919, 4000 samples / 8144
loss=3.356, 4800 samples / 8144
loss=3.767, 5600 samples / 8144
loss=3.667, 6400 samples / 8144
train accuracy = 0.23023084
test accuracy = 0.01719056974459725, test loss = 4.708303

epoch:  6 / 8
loss=2.379, 0 samples / 8144
loss=1.660, 800 samples / 8144
loss=2.227, 1600 samples / 8144
loss=2.716, 2400 samples / 8144
loss=1.712, 3200 samples / 8144
loss=1.160, 4000 samples / 8144
loss=2.452, 4800 samples / 8144
loss=1.695, 5600 samples / 8144
loss=2.078, 6400 samples / 8144
train accuracy = 0.47495088
test accuracy = 0.018418467583497054, test loss = 5.084738

epoch:  7 / 8
loss=0.376, 0 samples / 8144
loss=0.212, 800 samples / 8144
loss=0.410, 1600 samples / 8144
loss=1.296, 2400 samples / 8144
loss=1.385, 3200 samples / 8144
loss=1.268, 4000 samples / 8144
loss=0.274, 4800 samples / 8144
loss=0.465, 5600 samples / 8144
loss=0.453, 6400 samples / 8144
train accuracy = 0.66674853
test accuracy = 0.018909626719056976, test loss = 5.502324

epoch:  8 / 8
loss=0.059, 0 samples / 8144
loss=0.195, 800 samples / 8144
loss=0.352, 1600 samples / 8144
loss=0.648, 2400 samples / 8144
loss=0.586, 3200 samples / 8144
loss=0.469, 4000 samples / 8144
loss=0.307, 4800 samples / 8144
loss=0.131, 5600 samples / 8144
loss=0.138, 6400 samples / 8144
train accuracy = 0.73084479
test accuracy = 0.01792730844793713, test loss = 5.818804

fold number: 2
epoch:  1 / 8
loss=5.284, 0 samples / 8144
loss=5.260, 800 samples / 8144
loss=5.297, 1600 samples / 8144
loss=5.321, 2400 samples / 8144
loss=5.258, 3200 samples / 8144
loss=5.324, 4000 samples / 8144
loss=5.297, 4800 samples / 8144
loss=5.348, 5600 samples / 8144
loss=5.310, 6400 samples / 8144
train accuracy = 0.00429764
test accuracy = 0.0012278978388998035, test loss = 5.251862

epoch:  2 / 8
loss=5.233, 0 samples / 8144
loss=5.312, 800 samples / 8144
loss=5.198, 1600 samples / 8144
loss=5.215, 2400 samples / 8144
loss=4.754, 3200 samples / 8144
loss=5.209, 4000 samples / 8144
loss=5.256, 4800 samples / 8144
loss=5.054, 5600 samples / 8144
loss=5.253, 6400 samples / 8144
train accuracy = 0.01301572
test accuracy = 0.004052062868369352, test loss = 5.140954

epoch:  3 / 8
loss=4.781, 0 samples / 8144
loss=4.923, 800 samples / 8144
loss=4.761, 1600 samples / 8144
loss=5.071, 2400 samples / 8144
loss=4.922, 3200 samples / 8144
loss=4.596, 4000 samples / 8144
loss=4.785, 4800 samples / 8144
loss=4.556, 5600 samples / 8144
loss=4.681, 6400 samples / 8144
train accuracy = 0.03008350
test accuracy = 0.0063850687622789785, test loss = 4.993492

epoch:  4 / 8
loss=4.820, 0 samples / 8144
loss=4.716, 800 samples / 8144
loss=4.063, 1600 samples / 8144
loss=4.116, 2400 samples / 8144
loss=4.423, 3200 samples / 8144
loss=3.764, 4000 samples / 8144
loss=3.371, 4800 samples / 8144
loss=4.515, 5600 samples / 8144
loss=4.628, 6400 samples / 8144
train accuracy = 0.09405697
test accuracy = 0.012524557956777996, test loss = 4.774136

epoch:  5 / 8
loss=3.612, 0 samples / 8144
loss=2.965, 800 samples / 8144
loss=3.706, 1600 samples / 8144
loss=3.241, 2400 samples / 8144
loss=2.432, 3200 samples / 8144
loss=2.705, 4000 samples / 8144
loss=4.222, 4800 samples / 8144
loss=2.953, 5600 samples / 8144
loss=2.770, 6400 samples / 8144
train accuracy = 0.25503438
test accuracy = 0.015348722986247544, test loss = 4.829112

epoch:  6 / 8
loss=1.376, 0 samples / 8144
loss=1.069, 800 samples / 8144
loss=2.250, 1600 samples / 8144
loss=2.726, 2400 samples / 8144
loss=2.220, 3200 samples / 8144
loss=1.817, 4000 samples / 8144
loss=0.396, 4800 samples / 8144
loss=2.550, 5600 samples / 8144
loss=2.029, 6400 samples / 8144
train accuracy = 0.49791257
test accuracy = 0.01719056974459725, test loss = 5.223162

epoch:  7 / 8
loss=0.568, 0 samples / 8144
loss=0.808, 800 samples / 8144
loss=0.181, 1600 samples / 8144
loss=0.854, 2400 samples / 8144
loss=0.225, 3200 samples / 8144
loss=1.448, 4000 samples / 8144
loss=0.986, 4800 samples / 8144
loss=0.368, 5600 samples / 8144
loss=1.663, 6400 samples / 8144
train accuracy = 0.67227407
test accuracy = 0.01755893909626719, test loss = 5.744610

epoch:  8 / 8
loss=0.055, 0 samples / 8144
loss=0.158, 800 samples / 8144
loss=0.044, 1600 samples / 8144
loss=0.415, 2400 samples / 8144
loss=0.230, 3200 samples / 8144
loss=0.130, 4000 samples / 8144
loss=0.042, 4800 samples / 8144
loss=0.171, 5600 samples / 8144
loss=0.175, 6400 samples / 8144
train accuracy = 0.74557957
test accuracy = 0.01731335952848723, test loss = 6.125477

fold number: 3
epoch:  1 / 8
loss=5.342, 0 samples / 8144
loss=5.272, 800 samples / 8144
loss=5.359, 1600 samples / 8144
loss=5.293, 2400 samples / 8144
loss=5.252, 3200 samples / 8144
loss=5.290, 4000 samples / 8144
loss=5.277, 4800 samples / 8144
loss=5.245, 5600 samples / 8144
loss=5.300, 6400 samples / 8144
train accuracy = 0.00577112
test accuracy = 0.0022102161100196463, test loss = 5.259609

epoch:  2 / 8
loss=5.227, 0 samples / 8144
loss=5.199, 800 samples / 8144
loss=5.241, 1600 samples / 8144
loss=5.267, 2400 samples / 8144
loss=5.242, 3200 samples / 8144
loss=5.143, 4000 samples / 8144
loss=5.096, 4800 samples / 8144
loss=5.217, 5600 samples / 8144
loss=5.274, 6400 samples / 8144
train accuracy = 0.01129666
test accuracy = 0.00356090373280943, test loss = 5.156391

epoch:  3 / 8
loss=5.348, 0 samples / 8144
loss=4.879, 800 samples / 8144
loss=5.232, 1600 samples / 8144
loss=5.287, 2400 samples / 8144
loss=4.935, 3200 samples / 8144
loss=4.825, 4000 samples / 8144
loss=5.053, 4800 samples / 8144
loss=4.826, 5600 samples / 8144
loss=4.939, 6400 samples / 8144
train accuracy = 0.02173379
test accuracy = 0.006753438113948919, test loss = 5.017823

epoch:  4 / 8
loss=4.233, 0 samples / 8144
loss=4.653, 800 samples / 8144
loss=4.571, 1600 samples / 8144
loss=4.398, 2400 samples / 8144
loss=3.896, 3200 samples / 8144
loss=4.573, 4000 samples / 8144
loss=4.399, 4800 samples / 8144
loss=4.558, 5600 samples / 8144
loss=4.842, 6400 samples / 8144
train accuracy = 0.07170923
test accuracy = 0.011542239685658153, test loss = 4.796319

epoch:  5 / 8
loss=3.778, 0 samples / 8144
loss=2.775, 800 samples / 8144
loss=2.560, 1600 samples / 8144
loss=4.144, 2400 samples / 8144
loss=3.845, 3200 samples / 8144
loss=2.751, 4000 samples / 8144
loss=2.468, 4800 samples / 8144
loss=4.328, 5600 samples / 8144
loss=3.124, 6400 samples / 8144
train accuracy = 0.23010806
test accuracy = 0.015594302554027505, test loss = 4.764814

epoch:  6 / 8
loss=2.622, 0 samples / 8144
loss=1.865, 800 samples / 8144
loss=1.631, 1600 samples / 8144
loss=1.533, 2400 samples / 8144
loss=1.356, 3200 samples / 8144
loss=2.578, 4000 samples / 8144
loss=0.902, 4800 samples / 8144
loss=1.619, 5600 samples / 8144
loss=1.115, 6400 samples / 8144
train accuracy = 0.48944008
test accuracy = 0.016576620825147347, test loss = 5.183724

epoch:  7 / 8
loss=0.777, 0 samples / 8144
loss=0.355, 800 samples / 8144
loss=0.440, 1600 samples / 8144
loss=0.173, 2400 samples / 8144
loss=0.712, 3200 samples / 8144
loss=0.747, 4000 samples / 8144
loss=1.438, 4800 samples / 8144
loss=1.371, 5600 samples / 8144
loss=0.739, 6400 samples / 8144
train accuracy = 0.66613458
test accuracy = 0.01755893909626719, test loss = 5.696621

epoch:  8 / 8
loss=0.854, 0 samples / 8144
loss=0.357, 800 samples / 8144
loss=0.816, 1600 samples / 8144
loss=0.710, 2400 samples / 8144
loss=0.987, 3200 samples / 8144
loss=0.062, 4000 samples / 8144
loss=0.604, 4800 samples / 8144
loss=0.091, 5600 samples / 8144
loss=0.069, 6400 samples / 8144
train accuracy = 0.73612475
test accuracy = 0.019277996070726916, test loss = 6.007460

fold number: 4
epoch:  1 / 8
loss=5.275, 0 samples / 8144
loss=5.381, 800 samples / 8144
loss=5.370, 1600 samples / 8144
loss=5.298, 2400 samples / 8144
loss=5.245, 3200 samples / 8144
loss=5.225, 4000 samples / 8144
loss=5.234, 4800 samples / 8144
loss=5.220, 5600 samples / 8144
loss=5.281, 6400 samples / 8144
train accuracy = 0.00540275
test accuracy = 0.0019646365422396855, test loss = 5.219823

epoch:  2 / 8
loss=5.307, 0 samples / 8144
loss=5.295, 800 samples / 8144
loss=5.294, 1600 samples / 8144
loss=5.151, 2400 samples / 8144
loss=5.216, 3200 samples / 8144
loss=5.202, 4000 samples / 8144
loss=5.385, 4800 samples / 8144
loss=5.172, 5600 samples / 8144
loss=5.079, 6400 samples / 8144
train accuracy = 0.01191061
test accuracy = 0.00343811394891945, test loss = 5.137775

epoch:  3 / 8
loss=5.320, 0 samples / 8144
loss=5.088, 800 samples / 8144
loss=5.212, 1600 samples / 8144
loss=5.097, 2400 samples / 8144
loss=4.733, 3200 samples / 8144
loss=4.909, 4000 samples / 8144
loss=5.038, 4800 samples / 8144
loss=4.967, 5600 samples / 8144
loss=4.661, 6400 samples / 8144
train accuracy = 0.02308448
test accuracy = 0.007244597249508841, test loss = 5.009606

epoch:  4 / 8
loss=4.737, 0 samples / 8144
loss=5.107, 800 samples / 8144
loss=5.080, 1600 samples / 8144
loss=4.465, 2400 samples / 8144
loss=4.399, 3200 samples / 8144
loss=3.927, 4000 samples / 8144
loss=4.836, 4800 samples / 8144
loss=4.667, 5600 samples / 8144
loss=4.114, 6400 samples / 8144
train accuracy = 0.07305992
test accuracy = 0.012033398821218075, test loss = 4.835713

epoch:  5 / 8
loss=4.049, 0 samples / 8144
loss=5.175, 800 samples / 8144
loss=3.552, 1600 samples / 8144
loss=4.058, 2400 samples / 8144
loss=3.093, 3200 samples / 8144
loss=3.286, 4000 samples / 8144
loss=3.253, 4800 samples / 8144
loss=2.917, 5600 samples / 8144
loss=2.756, 6400 samples / 8144
train accuracy = 0.18934185
test accuracy = 0.016085461689587428, test loss = 4.771737

epoch:  6 / 8
loss=1.965, 0 samples / 8144
loss=2.419, 800 samples / 8144
loss=2.331, 1600 samples / 8144
loss=2.522, 2400 samples / 8144
loss=2.176, 3200 samples / 8144
loss=2.319, 4000 samples / 8144
loss=2.832, 4800 samples / 8144
loss=2.830, 5600 samples / 8144
loss=2.577, 6400 samples / 8144
train accuracy = 0.39857564
test accuracy = 0.018909626719056976, test loss = 4.981398

epoch:  7 / 8
loss=0.157, 0 samples / 8144
loss=0.608, 800 samples / 8144
loss=1.319, 1600 samples / 8144
loss=0.902, 2400 samples / 8144
loss=0.708, 3200 samples / 8144
loss=0.961, 4000 samples / 8144
loss=0.749, 4800 samples / 8144
loss=2.031, 5600 samples / 8144
loss=1.056, 6400 samples / 8144
train accuracy = 0.59847741
test accuracy = 0.019155206286836934, test loss = 5.412265

epoch:  8 / 8
loss=0.526, 0 samples / 8144
loss=1.113, 800 samples / 8144
loss=1.126, 1600 samples / 8144
loss=0.309, 2400 samples / 8144
loss=0.341, 3200 samples / 8144
loss=0.185, 4000 samples / 8144
loss=1.008, 4800 samples / 8144
loss=0.533, 5600 samples / 8144
loss=0.340, 6400 samples / 8144
train accuracy = 0.70419941
test accuracy = 0.01743614931237721, test loss = 5.967917

fold number: 5
epoch:  1 / 8
loss=5.267, 0 samples / 8144
loss=5.394, 800 samples / 8144
loss=5.308, 1600 samples / 8144
loss=5.269, 2400 samples / 8144
loss=5.323, 3200 samples / 8144
loss=5.273, 4000 samples / 8144
loss=5.266, 4800 samples / 8144
loss=5.290, 5600 samples / 8144
loss=5.284, 6400 samples / 8144
train accuracy = 0.00503438
test accuracy = 0.0022102161100196463, test loss = 5.242992

epoch:  2 / 8
loss=5.167, 0 samples / 8144
loss=5.276, 800 samples / 8144
loss=5.216, 1600 samples / 8144
loss=5.292, 2400 samples / 8144
loss=5.171, 3200 samples / 8144
loss=5.250, 4000 samples / 8144
loss=4.716, 4800 samples / 8144
loss=5.304, 5600 samples / 8144
loss=4.864, 6400 samples / 8144
train accuracy = 0.01227898
test accuracy = 0.004666011787819254, test loss = 5.148396

epoch:  3 / 8
loss=4.654, 0 samples / 8144
loss=4.527, 800 samples / 8144
loss=5.119, 1600 samples / 8144
loss=5.132, 2400 samples / 8144
loss=4.709, 3200 samples / 8144
loss=4.546, 4000 samples / 8144
loss=4.900, 4800 samples / 8144
loss=4.421, 5600 samples / 8144
loss=4.865, 6400 samples / 8144
train accuracy = 0.03388998
test accuracy = 0.007735756385068762, test loss = 4.949903

epoch:  4 / 8
loss=4.856, 0 samples / 8144
loss=4.033, 800 samples / 8144
loss=4.283, 1600 samples / 8144
loss=4.039, 2400 samples / 8144
loss=4.033, 3200 samples / 8144
loss=4.819, 4000 samples / 8144
loss=3.454, 4800 samples / 8144
loss=4.350, 5600 samples / 8144
loss=4.144, 6400 samples / 8144
train accuracy = 0.11014244
test accuracy = 0.013998035363457761, test loss = 4.750016

epoch:  5 / 8
loss=3.111, 0 samples / 8144
loss=3.160, 800 samples / 8144
loss=3.168, 1600 samples / 8144
loss=2.099, 2400 samples / 8144
loss=2.549, 3200 samples / 8144
loss=2.769, 4000 samples / 8144
loss=2.341, 4800 samples / 8144
loss=2.396, 5600 samples / 8144
loss=3.467, 6400 samples / 8144
train accuracy = 0.30206287
test accuracy = 0.016576620825147347, test loss = 4.821877

epoch:  6 / 8
loss=1.043, 0 samples / 8144
loss=2.247, 800 samples / 8144
loss=2.708, 1600 samples / 8144
loss=0.582, 2400 samples / 8144
loss=0.616, 3200 samples / 8144
loss=1.351, 4000 samples / 8144
loss=1.266, 4800 samples / 8144
loss=0.971, 5600 samples / 8144
loss=1.793, 6400 samples / 8144
train accuracy = 0.56262279
test accuracy = 0.018786836935166994, test loss = 5.229314

epoch:  7 / 8
loss=0.379, 0 samples / 8144
loss=0.443, 800 samples / 8144
loss=0.671, 1600 samples / 8144
loss=0.078, 2400 samples / 8144
loss=0.130, 3200 samples / 8144
loss=0.364, 4000 samples / 8144
loss=0.453, 4800 samples / 8144
loss=0.620, 5600 samples / 8144
loss=0.279, 6400 samples / 8144
train accuracy = 0.70358546
test accuracy = 0.01669941060903733, test loss = 5.841544

epoch:  8 / 8
loss=0.042, 0 samples / 8144
loss=0.069, 800 samples / 8144
loss=0.073, 1600 samples / 8144
loss=0.155, 2400 samples / 8144
loss=0.214, 3200 samples / 8144
loss=0.492, 4000 samples / 8144
loss=0.042, 4800 samples / 8144
loss=0.910, 5600 samples / 8144
loss=0.042, 6400 samples / 8144
train accuracy = 0.75380648
test accuracy = 0.01792730844793713, test loss = 6.150633

</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="31" id="vTpVsYUvZl-l">
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> get_mean_history(model_history, k_folds)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="32" id="NJOUL8Y7AiqA">
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>plot_accuracy(epochs, history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/adab536386ba6740ac2e906213dcd4983a09b5eb.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="33" id="pM_GADatDVJ2">
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>plot_loss(epochs, history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/ccf0fd69bc81f7e93bd3d7304c71f5339d1dc075.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="34" id="LYB_VZxrwWoE">
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>history_dataframe <span class="op">=</span> pd.DataFrame(history)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>history_dataframe</span></code></pre></div>
<div class="output execute_result" data-execution_count="34">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train_loss</th>
      <th>val_loss</th>
      <th>train_acc</th>
      <th>val_acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.282546</td>
      <td>5.244163</td>
      <td>0.004862</td>
      <td>0.001793</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5.187188</td>
      <td>5.146604</td>
      <td>0.012083</td>
      <td>0.003905</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.997819</td>
      <td>4.993814</td>
      <td>0.026989</td>
      <td>0.007146</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.413397</td>
      <td>4.784165</td>
      <td>0.084602</td>
      <td>0.012623</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.162488</td>
      <td>4.779169</td>
      <td>0.241356</td>
      <td>0.016159</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1.666070</td>
      <td>5.140467</td>
      <td>0.484700</td>
      <td>0.017976</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.697827</td>
      <td>5.639473</td>
      <td>0.661444</td>
      <td>0.017976</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.335619</td>
      <td>6.014058</td>
      <td>0.734111</td>
      <td>0.017976</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell markdown" id="bZ7gPhu-oYMT">
<p>From the results we gather that although our model is still overfitting, the validation accuracy has still improved substantially.</p>
</div>
<div class="cell markdown" id="2PsTWhkvoszL">
<p>Since our model is overfitting we can try:</p>
<ol>
<li><p>Adding more dropout layers in between convolution layers.</p></li>
<li><p>Decreasing the amount of convolutions per layer and add an additional convolution layer with dropout layers in between.</p></li>
<li><p>Decreasing the learning rate which may have caused us to "jump" over the absolute minimum leading to the best model convergence.</p></li>
</ol>
</div>
<section id="third-try" class="cell markdown" id="OUB57jD-Z9O5">
<h2>Third Try</h2>
</section>
<section id="defining-the-architecture" class="cell markdown" id="LoLrjRQpPntl">
<h3>Defining the architecture</h3>
</section>
<div class="cell code" id="euRNIkiFaBMp">
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNetV3(nn.Module):</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, in_height, in_width, max_pool_count):</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvNetV3, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>        out_height <span class="op">=</span> <span class="bu">int</span>(in_height <span class="op">/</span> (<span class="dv">2</span><span class="op">**</span>max_pool_count))</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>        out_width <span class="op">=</span> <span class="bu">int</span>(in_width <span class="op">/</span> (<span class="dv">2</span><span class="op">**</span>max_pool_count))</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>        out_channels <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flattened_dimensions <span class="op">=</span> out_channels <span class="op">*</span> out_height <span class="op">*</span> out_width</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># layers</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_channels, <span class="dv">64</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv21 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv22 <span class="op">=</span> nn.Conv2d(<span class="dv">128</span>, <span class="dv">128</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv23 <span class="op">=</span> nn.Conv2d(<span class="dv">128</span>, <span class="dv">128</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="va">self</span>.flattened_dimensions, <span class="dv">500</span>)</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">500</span>, <span class="dv">196</span>)</span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.25</span>)</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1st convolution</span></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv1(x)) <span class="co"># 3-&gt;64</span></span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv2(x)) <span class="co"># 64-&gt;64</span></span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv3(x)) <span class="co"># 64-&gt;64</span></span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># max pool</span></span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv21(x)) <span class="co"># 64-&gt;128</span></span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv22(x)) <span class="co"># 128-&gt;128</span></span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv23(x)) <span class="co"># 128-&gt;128</span></span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># max pool</span></span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># flatten image input</span></span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.flattened_dimensions)</span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-44"><a href="#cb64-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dropout</span></span>
<span id="cb64-45"><a href="#cb64-45" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb64-46"><a href="#cb64-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1st fully connected</span></span>
<span id="cb64-47"><a href="#cb64-47" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb64-48"><a href="#cb64-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dropout</span></span>
<span id="cb64-49"><a href="#cb64-49" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb64-50"><a href="#cb64-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-51"><a href="#cb64-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># predictions</span></span>
<span id="cb64-52"><a href="#cb64-52" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb64-53"><a href="#cb64-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-54"><a href="#cb64-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb64-55"><a href="#cb64-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-56"><a href="#cb64-56" aria-hidden="true" tabindex="-1"></a>channels, height, width <span class="op">=</span> picture_dim</span>
<span id="cb64-57"><a href="#cb64-57" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ConvNetV3(channels, height, width, <span class="dv">2</span>).to(device)</span>
<span id="cb64-58"><a href="#cb64-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-59"><a href="#cb64-59" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">5e-6</span></span>
<span id="cb64-60"><a href="#cb64-60" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb64-61"><a href="#cb64-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-62"><a href="#cb64-62" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">15</span></span></code></pre></div>
</div>
<section id="creating-augmentations" class="cell markdown" id="SnSw5xHNwZSH">
<h3>Creating augmentations</h3>
</section>
<div class="cell markdown" id="NiVdnRqYsqaU">
<p>We will save the augmented samples along with their original versions in a different folder so we can alter between datasets in case the results are not satisfying.</p>
</div>
<div class="cell code" id="DVtwTJd2sjXZ">
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>resize_images(train_path, train_annos_path, processed_train_path)</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:1255,&quot;timestamp&quot;:1670707720165}" id="vkh9dycxxEih" data-outputId="7d891ad7-65d1-4edb-d41a-e9252759f5af">
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>sample_img, label <span class="op">=</span> cropped_cars_train_dataset.<span class="fu">__getitem__</span>(<span class="dv">0</span>)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>sample_img <span class="op">=</span> sample_img.transpose()</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>sample_img <span class="op">=</span> sample_img.astype(np.uint8)</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> iaa.Sequential([iaa.AdditiveGaussianNoise(scale<span class="op">=</span><span class="fl">0.5</span><span class="op">*</span><span class="dv">255</span>)])</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>transformed_image <span class="op">=</span> transform(image<span class="op">=</span>sample_img)</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1</span>)</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>plt.imshow(sample_img)</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">2</span>)</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>plt.imshow(transformed_image)</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
</div>
<div class="cell code" id="OU7Xq7CIrgF1">
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> image_aug(index):</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>  sample_img, label <span class="op">=</span> cropped_cars_train_dataset.<span class="fu">__getitem__</span>(index)</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>  sample_img <span class="op">=</span> sample_img.transpose()</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>  sample_img <span class="op">=</span> sample_img.astype(np.uint8)</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>  transform <span class="op">=</span> iaa.Sequential([iaa.AdditiveGaussianNoise(scale<span class="op">=</span><span class="fl">0.5</span><span class="op">*</span><span class="dv">255</span>)])</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>  transformed_image <span class="op">=</span> transform(image<span class="op">=</span>sample_img)</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> transformed_image, label</span></code></pre></div>
</div>
<div class="cell code" id="FPRRAl7Ez-6W">
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>last_index <span class="op">=</span> <span class="bu">len</span>(y_train_labels)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>indexes <span class="op">=</span> random.sample(<span class="bu">range</span>(<span class="dv">0</span>,(last_index<span class="op">-</span><span class="dv">1</span>)), <span class="dv">3000</span>)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index <span class="kw">in</span> indexes:</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>  im_aug, label <span class="op">=</span> image_aug(index)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>  im_aug <span class="op">=</span> im_aug.astype(np.float32)</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>  image_name <span class="op">=</span> <span class="st">&quot;999&quot;</span> <span class="op">+</span> <span class="bu">str</span>(last_index) <span class="op">+</span> <span class="st">&quot;.jpg&quot;</span></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>  last_index <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>  cv2.imwrite(processed_train_path <span class="op">+</span> <span class="st">&quot;/&quot;</span> <span class="op">+</span> image_name, im_aug )</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>  df_aug <span class="op">=</span> pd.DataFrame([label])</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>  df_aug.to_csv(path_or_buf<span class="op">=</span>y_train_labels_path, mode<span class="op">=</span><span class="st">&#39;a&#39;</span>, index<span class="op">=</span><span class="va">False</span>, header<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:11,&quot;timestamp&quot;:1670707797744}" id="2VDrdduw0-Um" data-outputId="859713bd-0770-4ae4-85ab-36d59d8200a4">
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>image_names <span class="op">=</span> <span class="bu">sorted</span>(os.listdir(processed_train_path))</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(image_names)</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:901,&quot;timestamp&quot;:1670707798639}" id="JWlZQATO1TYk" data-outputId="acbd9434-9215-4346-cbb0-a41750e89819">
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>processed_cars_train_dataset <span class="op">=</span> cars_dataset(processed_train_path, y_train_labels_path)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>transformed_image, label <span class="op">=</span> processed_cars_train_dataset.<span class="fu">__getitem__</span>(<span class="dv">11120</span>)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>transformed_image <span class="op">=</span> transformed_image.transpose()</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>transformed_image <span class="op">=</span> transformed_image.astype(np.uint8)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1</span>)</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(transformed_image)</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
</div>
<section id="results" class="cell markdown" id="W7eTqARnaDIX">
<h3>Results</h3>
</section>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="qwC8c7qqaHhh" data-outputId="45f42946-283d-4678-f438-a87b68b9150f">
<div class="sourceCode" id="cb71"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>model_history <span class="op">=</span> train_and_validate_kfold(model, processed_cars_train_dataset, epochs<span class="op">=</span>epochs, batch_size<span class="op">=</span>batch_size, learning_rate<span class="op">=</span>learning_rate, loss_fn<span class="op">=</span>loss_fn)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>fold number: 1
epoch:  1 / 15
loss=5.277, 0 samples / 11144
loss=5.421, 800 samples / 11144
loss=5.252, 1600 samples / 11144
loss=5.329, 2400 samples / 11144
loss=5.300, 3200 samples / 11144
loss=5.276, 4000 samples / 11144
loss=5.266, 4800 samples / 11144
loss=5.273, 5600 samples / 11144
loss=5.297, 6400 samples / 11144
loss=5.270, 7200 samples / 11144
loss=5.274, 8000 samples / 11144
loss=5.298, 8800 samples / 11144
train accuracy = 0.00332017
test accuracy = 0.0011665470208183776, test loss = 5.276884

epoch:  2 / 15
loss=5.276, 0 samples / 11144
loss=5.278, 800 samples / 11144
loss=5.270, 1600 samples / 11144
loss=5.270, 2400 samples / 11144
loss=5.303, 3200 samples / 11144
loss=5.293, 4000 samples / 11144
loss=5.178, 4800 samples / 11144
loss=5.265, 5600 samples / 11144
loss=5.214, 6400 samples / 11144
loss=5.242, 7200 samples / 11144
loss=5.348, 8000 samples / 11144
loss=5.206, 8800 samples / 11144
train accuracy = 0.00457645
test accuracy = 0.0021536252692031586, test loss = 5.254274

epoch:  3 / 15
loss=5.067, 0 samples / 11144
loss=5.209, 800 samples / 11144
loss=5.114, 1600 samples / 11144
loss=5.282, 2400 samples / 11144
loss=5.327, 3200 samples / 11144
loss=5.293, 4000 samples / 11144
loss=5.261, 4800 samples / 11144
loss=5.193, 5600 samples / 11144
loss=4.887, 6400 samples / 11144
loss=5.275, 7200 samples / 11144
loss=5.359, 8000 samples / 11144
loss=5.164, 8800 samples / 11144
train accuracy = 0.00699928
test accuracy = 0.0037688442211055275, test loss = 5.218667

epoch:  4 / 15
loss=5.309, 0 samples / 11144
loss=5.025, 800 samples / 11144
loss=5.114, 1600 samples / 11144
loss=5.038, 2400 samples / 11144
loss=5.372, 3200 samples / 11144
loss=5.262, 4000 samples / 11144
loss=5.375, 4800 samples / 11144
loss=5.126, 5600 samples / 11144
loss=4.945, 6400 samples / 11144
loss=5.176, 7200 samples / 11144
loss=5.147, 8000 samples / 11144
loss=5.081, 8800 samples / 11144
train accuracy = 0.01749821
test accuracy = 0.005473797559224695, test loss = 5.138371

epoch:  5 / 15
loss=5.063, 0 samples / 11144
loss=4.739, 800 samples / 11144
loss=5.469, 1600 samples / 11144
loss=3.581, 2400 samples / 11144
loss=4.266, 3200 samples / 11144
loss=4.845, 4000 samples / 11144
loss=4.551, 4800 samples / 11144
loss=4.686, 5600 samples / 11144
loss=4.575, 6400 samples / 11144
loss=4.038, 7200 samples / 11144
loss=4.211, 8000 samples / 11144
loss=5.955, 8800 samples / 11144
train accuracy = 0.07708184
test accuracy = 0.006999282124910265, test loss = 5.128787

epoch:  6 / 15
loss=3.704, 0 samples / 11144
loss=3.361, 800 samples / 11144
loss=2.779, 1600 samples / 11144
loss=3.113, 2400 samples / 11144
loss=2.272, 3200 samples / 11144
loss=1.394, 4000 samples / 11144
loss=3.150, 4800 samples / 11144
loss=3.335, 5600 samples / 11144
loss=2.688, 6400 samples / 11144
loss=2.548, 7200 samples / 11144
loss=2.327, 8000 samples / 11144
loss=2.070, 8800 samples / 11144
train accuracy = 0.29737976
test accuracy = 0.007986360373295047, test loss = 5.810860

epoch:  7 / 15
loss=2.000, 0 samples / 11144
loss=2.138, 800 samples / 11144
loss=1.385, 1600 samples / 11144
loss=0.805, 2400 samples / 11144
loss=1.186, 3200 samples / 11144
loss=2.494, 4000 samples / 11144
loss=0.828, 4800 samples / 11144
loss=0.927, 5600 samples / 11144
loss=1.134, 6400 samples / 11144
loss=1.709, 7200 samples / 11144
loss=1.202, 8000 samples / 11144
loss=0.982, 8800 samples / 11144
train accuracy = 0.59108040
test accuracy = 0.00780689160086145, test loss = 6.919808

epoch:  8 / 15
loss=0.512, 0 samples / 11144
loss=0.644, 800 samples / 11144
loss=0.280, 1600 samples / 11144
loss=1.015, 2400 samples / 11144
loss=0.656, 3200 samples / 11144
loss=0.067, 4000 samples / 11144
loss=0.216, 4800 samples / 11144
loss=0.016, 5600 samples / 11144
loss=0.072, 6400 samples / 11144
loss=0.249, 7200 samples / 11144
loss=0.557, 8000 samples / 11144
loss=0.289, 8800 samples / 11144
train accuracy = 0.74748744
test accuracy = 0.00780689160086145, test loss = 7.542490

epoch:  9 / 15
loss=0.129, 0 samples / 11144
loss=0.142, 800 samples / 11144
loss=0.037, 1600 samples / 11144
loss=0.080, 2400 samples / 11144
loss=0.057, 3200 samples / 11144
loss=0.236, 4000 samples / 11144
loss=0.659, 4800 samples / 11144
loss=0.023, 5600 samples / 11144
loss=0.038, 6400 samples / 11144
loss=0.039, 7200 samples / 11144
loss=0.273, 8000 samples / 11144
loss=0.887, 8800 samples / 11144
train accuracy = 0.77090811
test accuracy = 0.007537688442211055, test loss = 8.189412

epoch:  10 / 15
loss=0.017, 0 samples / 11144
loss=0.041, 800 samples / 11144
loss=0.136, 1600 samples / 11144
loss=0.024, 2400 samples / 11144
loss=0.532, 3200 samples / 11144
loss=0.028, 4000 samples / 11144
loss=0.101, 4800 samples / 11144
loss=0.208, 5600 samples / 11144
loss=0.186, 6400 samples / 11144
loss=0.055, 7200 samples / 11144
loss=0.220, 8000 samples / 11144
loss=0.364, 8800 samples / 11144
train accuracy = 0.77763819
test accuracy = 0.008255563531945441, test loss = 8.385319

epoch:  11 / 15
loss=0.013, 0 samples / 11144
loss=0.009, 800 samples / 11144
loss=0.011, 1600 samples / 11144
loss=0.004, 2400 samples / 11144
loss=0.192, 3200 samples / 11144
loss=0.150, 4000 samples / 11144
loss=0.007, 4800 samples / 11144
loss=0.091, 5600 samples / 11144
loss=0.130, 6400 samples / 11144
loss=0.061, 7200 samples / 11144
loss=0.040, 8000 samples / 11144
loss=0.095, 8800 samples / 11144
train accuracy = 0.78266332
test accuracy = 0.007627422828427853, test loss = 8.726069

epoch:  12 / 15
loss=0.052, 0 samples / 11144
loss=0.024, 800 samples / 11144
loss=0.072, 1600 samples / 11144
loss=0.005, 2400 samples / 11144
loss=0.071, 3200 samples / 11144
loss=0.008, 4000 samples / 11144
loss=0.015, 4800 samples / 11144
loss=0.033, 5600 samples / 11144
loss=0.027, 6400 samples / 11144
loss=0.028, 7200 samples / 11144
loss=0.007, 8000 samples / 11144
loss=0.071, 8800 samples / 11144
train accuracy = 0.78759871
test accuracy = 0.007537688442211055, test loss = 8.774915

epoch:  13 / 15
loss=0.064, 0 samples / 11144
loss=0.077, 800 samples / 11144
loss=0.012, 1600 samples / 11144
loss=0.001, 2400 samples / 11144
loss=0.040, 3200 samples / 11144
loss=0.143, 4000 samples / 11144
loss=0.063, 4800 samples / 11144
loss=0.004, 5600 samples / 11144
loss=0.023, 6400 samples / 11144
loss=0.007, 7200 samples / 11144
loss=0.003, 8000 samples / 11144
loss=0.025, 8800 samples / 11144
train accuracy = 0.78921393
test accuracy = 0.008165829145728644, test loss = 9.103277

epoch:  14 / 15
loss=0.009, 0 samples / 11144
loss=0.001, 800 samples / 11144
loss=0.039, 1600 samples / 11144
loss=0.010, 2400 samples / 11144
loss=0.501, 3200 samples / 11144
loss=0.001, 4000 samples / 11144
loss=0.009, 4800 samples / 11144
loss=0.128, 5600 samples / 11144
loss=0.088, 6400 samples / 11144
loss=0.015, 7200 samples / 11144
loss=0.034, 8000 samples / 11144
loss=0.023, 8800 samples / 11144
train accuracy = 0.78867552
test accuracy = 0.0070890165111270635, test loss = 8.679888

epoch:  15 / 15
loss=0.012, 0 samples / 11144
loss=0.002, 800 samples / 11144
loss=0.025, 1600 samples / 11144
loss=0.075, 2400 samples / 11144
loss=0.002, 3200 samples / 11144
loss=0.002, 4000 samples / 11144
loss=0.149, 4800 samples / 11144
loss=0.006, 5600 samples / 11144
loss=0.084, 6400 samples / 11144
loss=0.048, 7200 samples / 11144
loss=0.011, 8000 samples / 11144
loss=0.026, 8800 samples / 11144
train accuracy = 0.79190596
test accuracy = 0.006640344580043073, test loss = 8.517132

fold number: 2
epoch:  1 / 15
loss=5.318, 0 samples / 11144
loss=5.232, 800 samples / 11144
loss=5.229, 1600 samples / 11144
loss=5.272, 2400 samples / 11144
loss=5.301, 3200 samples / 11144
loss=5.272, 4000 samples / 11144
loss=5.297, 4800 samples / 11144
loss=5.273, 5600 samples / 11144
loss=5.266, 6400 samples / 11144
loss=5.284, 7200 samples / 11144
loss=5.271, 8000 samples / 11144
loss=5.277, 8800 samples / 11144
train accuracy = 0.00394831
test accuracy = 0.001256281407035176, test loss = 5.279533

epoch:  2 / 15
loss=5.270, 0 samples / 11144
loss=5.283, 800 samples / 11144
loss=5.272, 1600 samples / 11144
loss=5.280, 2400 samples / 11144
loss=5.300, 3200 samples / 11144
loss=5.288, 4000 samples / 11144
loss=5.276, 4800 samples / 11144
loss=5.299, 5600 samples / 11144
loss=5.268, 6400 samples / 11144
loss=5.297, 7200 samples / 11144
loss=5.285, 8000 samples / 11144
loss=5.219, 8800 samples / 11144
train accuracy = 0.00475592
test accuracy = 0.000987078248384781, test loss = 5.276817

epoch:  3 / 15
loss=5.230, 0 samples / 11144
loss=5.274, 800 samples / 11144
loss=5.179, 1600 samples / 11144
loss=5.302, 2400 samples / 11144
loss=5.306, 3200 samples / 11144
loss=5.261, 4000 samples / 11144
loss=5.190, 4800 samples / 11144
loss=5.250, 5600 samples / 11144
loss=5.293, 6400 samples / 11144
loss=5.290, 7200 samples / 11144
loss=5.231, 8000 samples / 11144
loss=5.282, 8800 samples / 11144
train accuracy = 0.00664034
test accuracy = 0.00314070351758794, test loss = 5.229287

epoch:  4 / 15
loss=5.277, 0 samples / 11144
loss=5.132, 800 samples / 11144
loss=5.293, 1600 samples / 11144
loss=5.294, 2400 samples / 11144
loss=5.181, 3200 samples / 11144
loss=5.153, 4000 samples / 11144
loss=5.198, 4800 samples / 11144
loss=4.950, 5600 samples / 11144
loss=5.138, 6400 samples / 11144
loss=5.258, 7200 samples / 11144
loss=5.306, 8000 samples / 11144
loss=5.151, 8800 samples / 11144
train accuracy = 0.01767767
test accuracy = 0.0052943287867910985, test loss = 5.151909

epoch:  5 / 15
loss=5.153, 0 samples / 11144
loss=5.235, 800 samples / 11144
loss=4.665, 1600 samples / 11144
loss=4.728, 2400 samples / 11144
loss=4.677, 3200 samples / 11144
loss=4.788, 4000 samples / 11144
loss=4.506, 4800 samples / 11144
loss=5.006, 5600 samples / 11144
loss=5.214, 6400 samples / 11144
loss=4.637, 7200 samples / 11144
loss=4.184, 8000 samples / 11144
loss=4.994, 8800 samples / 11144
train accuracy = 0.05743001
test accuracy = 0.008076094759511845, test loss = 5.077411

epoch:  6 / 15
loss=4.135, 0 samples / 11144
loss=3.616, 800 samples / 11144
loss=3.402, 1600 samples / 11144
loss=3.208, 2400 samples / 11144
loss=3.074, 3200 samples / 11144
loss=3.953, 4000 samples / 11144
loss=2.941, 4800 samples / 11144
loss=3.215, 5600 samples / 11144
loss=4.011, 6400 samples / 11144
loss=3.221, 7200 samples / 11144
loss=2.719, 8000 samples / 11144
loss=3.174, 8800 samples / 11144
train accuracy = 0.25188442
test accuracy = 0.009691313711414214, test loss = 5.617099

epoch:  7 / 15
loss=2.367, 0 samples / 11144
loss=0.702, 800 samples / 11144
loss=1.133, 1600 samples / 11144
loss=1.908, 2400 samples / 11144
loss=0.944, 3200 samples / 11144
loss=0.532, 4000 samples / 11144
loss=1.608, 4800 samples / 11144
loss=0.320, 5600 samples / 11144
loss=1.622, 6400 samples / 11144
loss=1.072, 7200 samples / 11144
loss=1.419, 8000 samples / 11144
loss=2.075, 8800 samples / 11144
train accuracy = 0.55007179
test accuracy = 0.011665470208183776, test loss = 6.853094

epoch:  8 / 15
loss=0.043, 0 samples / 11144
loss=0.683, 800 samples / 11144
loss=1.095, 1600 samples / 11144
loss=0.211, 2400 samples / 11144
loss=0.118, 3200 samples / 11144
loss=0.368, 4000 samples / 11144
loss=0.423, 4800 samples / 11144
loss=0.529, 5600 samples / 11144
loss=0.194, 6400 samples / 11144
loss=0.425, 7200 samples / 11144
loss=0.039, 8000 samples / 11144
loss=0.183, 8800 samples / 11144
train accuracy = 0.73519383
test accuracy = 0.008524766690595837, test loss = 8.063106

epoch:  9 / 15
loss=0.036, 0 samples / 11144
loss=0.273, 800 samples / 11144
loss=0.077, 1600 samples / 11144
loss=0.049, 2400 samples / 11144
loss=0.011, 3200 samples / 11144
loss=0.037, 4000 samples / 11144
loss=0.330, 4800 samples / 11144
loss=0.219, 5600 samples / 11144
loss=0.091, 6400 samples / 11144
loss=0.202, 7200 samples / 11144
loss=0.191, 8000 samples / 11144
loss=0.247, 8800 samples / 11144
train accuracy = 0.77611271
test accuracy = 0.0104091888011486, test loss = 8.302260

epoch:  10 / 15
loss=0.012, 0 samples / 11144
loss=0.005, 800 samples / 11144
loss=0.540, 1600 samples / 11144
loss=0.183, 2400 samples / 11144
loss=0.243, 3200 samples / 11144
loss=0.189, 4000 samples / 11144
loss=0.101, 4800 samples / 11144
loss=0.019, 5600 samples / 11144
loss=0.048, 6400 samples / 11144
loss=0.399, 7200 samples / 11144
loss=0.048, 8000 samples / 11144
loss=0.191, 8800 samples / 11144
train accuracy = 0.78212491
test accuracy = 0.008614501076812634, test loss = 8.686205

epoch:  11 / 15
loss=0.035, 0 samples / 11144
loss=0.006, 800 samples / 11144
loss=0.002, 1600 samples / 11144
loss=0.013, 2400 samples / 11144
loss=0.006, 3200 samples / 11144
loss=0.046, 4000 samples / 11144
loss=1.395, 4800 samples / 11144
loss=0.008, 5600 samples / 11144
loss=0.001, 6400 samples / 11144
loss=0.160, 7200 samples / 11144
loss=0.094, 8000 samples / 11144
loss=0.066, 8800 samples / 11144
train accuracy = 0.78598349
test accuracy = 0.008973438621679828, test loss = 9.344480

epoch:  12 / 15
loss=0.045, 0 samples / 11144
loss=0.007, 800 samples / 11144
loss=0.008, 1600 samples / 11144
loss=0.130, 2400 samples / 11144
loss=0.184, 3200 samples / 11144
loss=0.054, 4000 samples / 11144
loss=0.046, 4800 samples / 11144
loss=0.148, 5600 samples / 11144
loss=0.029, 6400 samples / 11144
loss=0.038, 7200 samples / 11144
loss=0.097, 8000 samples / 11144
loss=0.028, 8800 samples / 11144
train accuracy = 0.78634243
test accuracy = 0.009691313711414214, test loss = 9.122482

epoch:  13 / 15
loss=0.349, 0 samples / 11144
loss=0.031, 800 samples / 11144
loss=0.014, 1600 samples / 11144
loss=0.120, 2400 samples / 11144
loss=0.022, 3200 samples / 11144
loss=0.101, 4000 samples / 11144
loss=0.003, 4800 samples / 11144
loss=0.025, 5600 samples / 11144
loss=0.028, 6400 samples / 11144
loss=0.002, 7200 samples / 11144
loss=0.013, 8000 samples / 11144
loss=0.151, 8800 samples / 11144
train accuracy = 0.78984207
test accuracy = 0.010229720028715004, test loss = 8.806100

epoch:  14 / 15
loss=0.033, 0 samples / 11144
loss=0.018, 800 samples / 11144
loss=0.020, 1600 samples / 11144
loss=0.010, 2400 samples / 11144
loss=0.205, 3200 samples / 11144
loss=0.010, 4000 samples / 11144
loss=0.001, 4800 samples / 11144
loss=0.001, 5600 samples / 11144
loss=0.007, 6400 samples / 11144
loss=0.008, 7200 samples / 11144
loss=0.008, 8000 samples / 11144
loss=0.002, 8800 samples / 11144
train accuracy = 0.78939340
test accuracy = 0.009601579325197415, test loss = 9.124044

epoch:  15 / 15
loss=0.003, 0 samples / 11144
loss=0.011, 800 samples / 11144
loss=0.003, 1600 samples / 11144
loss=0.000, 2400 samples / 11144
loss=0.009, 3200 samples / 11144
loss=0.013, 4000 samples / 11144
loss=0.028, 4800 samples / 11144
loss=0.009, 5600 samples / 11144
loss=0.184, 6400 samples / 11144
loss=0.002, 7200 samples / 11144
loss=0.023, 8000 samples / 11144
loss=0.007, 8800 samples / 11144
train accuracy = 0.79244436
test accuracy = 0.009781048097631011, test loss = 9.519102

fold number: 3
epoch:  1 / 15
loss=5.265, 0 samples / 11144
loss=5.240, 800 samples / 11144
loss=5.335, 1600 samples / 11144
loss=5.297, 2400 samples / 11144
loss=5.265, 3200 samples / 11144
loss=5.275, 4000 samples / 11144
loss=5.278, 4800 samples / 11144
loss=5.260, 5600 samples / 11144
loss=5.283, 6400 samples / 11144
loss=5.290, 7200 samples / 11144
loss=5.283, 8000 samples / 11144
loss=5.288, 8800 samples / 11144
train accuracy = 0.00394831
test accuracy = 0.0003589375448671931, test loss = 5.278436

epoch:  2 / 15
loss=5.265, 0 samples / 11144
loss=5.287, 800 samples / 11144
loss=5.279, 1600 samples / 11144
loss=5.284, 2400 samples / 11144
loss=5.271, 3200 samples / 11144
loss=5.266, 4000 samples / 11144
loss=5.292, 4800 samples / 11144
loss=5.278, 5600 samples / 11144
loss=5.284, 6400 samples / 11144
loss=5.285, 7200 samples / 11144
loss=5.273, 8000 samples / 11144
loss=5.269, 8800 samples / 11144
train accuracy = 0.00394831
test accuracy = 0.001256281407035176, test loss = 5.278416

epoch:  3 / 15
loss=5.267, 0 samples / 11144
loss=5.273, 800 samples / 11144
loss=5.266, 1600 samples / 11144
loss=5.414, 2400 samples / 11144
loss=5.278, 3200 samples / 11144
loss=5.284, 4000 samples / 11144
loss=5.295, 4800 samples / 11144
loss=5.295, 5600 samples / 11144
loss=5.303, 6400 samples / 11144
loss=5.273, 7200 samples / 11144
loss=5.273, 8000 samples / 11144
loss=5.250, 8800 samples / 11144
train accuracy = 0.00403805
test accuracy = 0.0008076094759511845, test loss = 5.278906

epoch:  4 / 15
loss=5.288, 0 samples / 11144
loss=5.291, 800 samples / 11144
loss=5.285, 1600 samples / 11144
loss=5.226, 2400 samples / 11144
loss=5.254, 3200 samples / 11144
loss=5.256, 4000 samples / 11144
loss=5.322, 4800 samples / 11144
loss=5.190, 5600 samples / 11144
loss=5.266, 6400 samples / 11144
loss=5.262, 7200 samples / 11144
loss=5.292, 8000 samples / 11144
loss=5.253, 8800 samples / 11144
train accuracy = 0.00870424
test accuracy = 0.0021536252692031586, test loss = 5.221107

epoch:  5 / 15
loss=5.284, 0 samples / 11144
loss=5.245, 800 samples / 11144
loss=5.183, 1600 samples / 11144
loss=5.163, 2400 samples / 11144
loss=5.208, 3200 samples / 11144
loss=5.518, 4000 samples / 11144
loss=5.215, 4800 samples / 11144
loss=5.146, 5600 samples / 11144
loss=5.367, 6400 samples / 11144
loss=4.821, 7200 samples / 11144
loss=5.232, 8000 samples / 11144
loss=4.950, 8800 samples / 11144
train accuracy = 0.01929289
test accuracy = 0.005384063173007897, test loss = 5.142563

epoch:  6 / 15
loss=4.840, 0 samples / 11144
loss=4.560, 800 samples / 11144
loss=4.150, 1600 samples / 11144
loss=4.585, 2400 samples / 11144
loss=4.281, 3200 samples / 11144
loss=3.863, 4000 samples / 11144
loss=4.324, 4800 samples / 11144
loss=3.904, 5600 samples / 11144
loss=4.756, 6400 samples / 11144
loss=4.172, 7200 samples / 11144
loss=4.071, 8000 samples / 11144
loss=4.225, 8800 samples / 11144
train accuracy = 0.09888729
test accuracy = 0.009063173007896627, test loss = 5.143095

epoch:  7 / 15
loss=2.537, 0 samples / 11144
loss=3.242, 800 samples / 11144
loss=2.180, 1600 samples / 11144
loss=1.485, 2400 samples / 11144
loss=3.187, 3200 samples / 11144
loss=1.959, 4000 samples / 11144
loss=2.462, 4800 samples / 11144
loss=1.539, 5600 samples / 11144
loss=2.032, 6400 samples / 11144
loss=2.945, 7200 samples / 11144
loss=4.164, 8000 samples / 11144
loss=1.906, 8800 samples / 11144
train accuracy = 0.38783202
test accuracy = 0.009422110552763818, test loss = 5.909495

epoch:  8 / 15
loss=0.802, 0 samples / 11144
loss=1.485, 800 samples / 11144
loss=0.125, 1600 samples / 11144
loss=0.068, 2400 samples / 11144
loss=0.049, 3200 samples / 11144
loss=0.124, 4000 samples / 11144
loss=0.300, 4800 samples / 11144
loss=0.557, 5600 samples / 11144
loss=0.376, 6400 samples / 11144
loss=3.247, 7200 samples / 11144
loss=0.293, 8000 samples / 11144
loss=0.492, 8800 samples / 11144
train accuracy = 0.70163317
test accuracy = 0.008435032304379038, test loss = 7.051641

epoch:  9 / 15
loss=0.050, 0 samples / 11144
loss=0.338, 800 samples / 11144
loss=0.009, 1600 samples / 11144
loss=0.135, 2400 samples / 11144
loss=0.154, 3200 samples / 11144
loss=0.032, 4000 samples / 11144
loss=0.037, 4800 samples / 11144
loss=0.210, 5600 samples / 11144
loss=0.059, 6400 samples / 11144
loss=0.020, 7200 samples / 11144
loss=0.070, 8000 samples / 11144
loss=0.019, 8800 samples / 11144
train accuracy = 0.77189519
test accuracy = 0.00834529791816224, test loss = 8.090531

epoch:  10 / 15
loss=0.084, 0 samples / 11144
loss=0.053, 800 samples / 11144
loss=0.083, 1600 samples / 11144
loss=0.068, 2400 samples / 11144
loss=0.091, 3200 samples / 11144
loss=0.019, 4000 samples / 11144
loss=0.053, 4800 samples / 11144
loss=0.033, 5600 samples / 11144
loss=2.508, 6400 samples / 11144
loss=0.155, 7200 samples / 11144
loss=0.042, 8000 samples / 11144
loss=0.074, 8800 samples / 11144
train accuracy = 0.78140704
test accuracy = 0.008435032304379038, test loss = 8.163018

epoch:  11 / 15
loss=0.316, 0 samples / 11144
loss=0.066, 800 samples / 11144
loss=0.043, 1600 samples / 11144
loss=0.064, 2400 samples / 11144
loss=0.024, 3200 samples / 11144
loss=0.007, 4000 samples / 11144
loss=0.030, 4800 samples / 11144
loss=0.072, 5600 samples / 11144
loss=0.018, 6400 samples / 11144
loss=0.017, 7200 samples / 11144
loss=0.031, 8000 samples / 11144
loss=0.014, 8800 samples / 11144
train accuracy = 0.78472721
test accuracy = 0.007178750897343862, test loss = 8.527646

epoch:  12 / 15
loss=0.401, 0 samples / 11144
loss=0.007, 800 samples / 11144
loss=0.119, 1600 samples / 11144
loss=0.010, 2400 samples / 11144
loss=0.029, 3200 samples / 11144
loss=1.755, 4000 samples / 11144
loss=0.038, 4800 samples / 11144
loss=0.037, 5600 samples / 11144
loss=0.070, 6400 samples / 11144
loss=0.137, 7200 samples / 11144
loss=0.004, 8000 samples / 11144
loss=0.017, 8800 samples / 11144
train accuracy = 0.78688083
test accuracy = 0.00933237616654702, test loss = 8.193791

epoch:  13 / 15
loss=0.005, 0 samples / 11144
loss=0.002, 800 samples / 11144
loss=0.025, 1600 samples / 11144
loss=0.007, 2400 samples / 11144
loss=0.013, 3200 samples / 11144
loss=0.006, 4000 samples / 11144
loss=0.003, 4800 samples / 11144
loss=0.000, 5600 samples / 11144
loss=0.041, 6400 samples / 11144
loss=0.000, 7200 samples / 11144
loss=0.845, 8000 samples / 11144
loss=0.631, 8800 samples / 11144
train accuracy = 0.78957286
test accuracy = 0.00888370423546303, test loss = 8.414928

epoch:  14 / 15
loss=0.118, 0 samples / 11144
loss=0.009, 800 samples / 11144
loss=0.034, 1600 samples / 11144
loss=0.008, 2400 samples / 11144
loss=0.001, 3200 samples / 11144
loss=0.140, 4000 samples / 11144
loss=0.002, 4800 samples / 11144
loss=0.023, 5600 samples / 11144
loss=0.001, 6400 samples / 11144
loss=0.125, 7200 samples / 11144
loss=0.012, 8000 samples / 11144
loss=0.093, 8800 samples / 11144
train accuracy = 0.79047021
test accuracy = 0.008614501076812634, test loss = 8.303594

epoch:  15 / 15
loss=0.000, 0 samples / 11144
loss=0.009, 800 samples / 11144
loss=0.004, 1600 samples / 11144
loss=3.409, 2400 samples / 11144
loss=0.376, 3200 samples / 11144
loss=0.002, 4000 samples / 11144
loss=0.044, 4800 samples / 11144
loss=0.001, 5600 samples / 11144
loss=0.019, 6400 samples / 11144
loss=0.009, 7200 samples / 11144
loss=0.008, 8000 samples / 11144
loss=0.026, 8800 samples / 11144
train accuracy = 0.79316224
test accuracy = 0.00888370423546303, test loss = 8.340775

fold number: 4
epoch:  1 / 15
loss=5.188, 0 samples / 11144
loss=5.213, 800 samples / 11144
loss=5.253, 1600 samples / 11144
loss=5.283, 2400 samples / 11144
loss=5.267, 3200 samples / 11144
loss=5.282, 4000 samples / 11144
loss=5.273, 4800 samples / 11144
loss=5.294, 5600 samples / 11144
loss=5.293, 6400 samples / 11144
loss=5.259, 7200 samples / 11144
loss=5.249, 8000 samples / 11144
loss=5.246, 8800 samples / 11144
train accuracy = 0.00439698
test accuracy = 0.0017049533381191674, test loss = 5.270740

epoch:  2 / 15
loss=5.272, 0 samples / 11144
loss=5.279, 800 samples / 11144
loss=5.251, 1600 samples / 11144
loss=5.279, 2400 samples / 11144
loss=5.274, 3200 samples / 11144
loss=5.247, 4000 samples / 11144
loss=5.279, 4800 samples / 11144
loss=5.268, 5600 samples / 11144
loss=5.294, 6400 samples / 11144
loss=5.303, 7200 samples / 11144
loss=5.236, 8000 samples / 11144
loss=5.290, 8800 samples / 11144
train accuracy = 0.00565327
test accuracy = 0.0027817659727207467, test loss = 5.246825

epoch:  3 / 15
loss=5.220, 0 samples / 11144
loss=5.102, 800 samples / 11144
loss=5.120, 1600 samples / 11144
loss=5.100, 2400 samples / 11144
loss=5.248, 3200 samples / 11144
loss=5.367, 4000 samples / 11144
loss=5.148, 4800 samples / 11144
loss=5.320, 5600 samples / 11144
loss=4.808, 6400 samples / 11144
loss=5.370, 7200 samples / 11144
loss=5.094, 8000 samples / 11144
loss=5.153, 8800 samples / 11144
train accuracy = 0.01722900
test accuracy = 0.004217516152189519, test loss = 5.179104

epoch:  4 / 15
loss=4.921, 0 samples / 11144
loss=5.069, 800 samples / 11144
loss=5.015, 1600 samples / 11144
loss=4.321, 2400 samples / 11144
loss=4.691, 3200 samples / 11144
loss=4.412, 4000 samples / 11144
loss=4.632, 4800 samples / 11144
loss=4.458, 5600 samples / 11144
loss=4.340, 6400 samples / 11144
loss=4.184, 7200 samples / 11144
loss=4.483, 8000 samples / 11144
loss=4.578, 8800 samples / 11144
train accuracy = 0.07214645
test accuracy = 0.008614501076812634, test loss = 5.107972

epoch:  5 / 15
loss=3.438, 0 samples / 11144
loss=2.097, 800 samples / 11144
loss=2.548, 1600 samples / 11144
loss=2.689, 2400 samples / 11144
loss=3.070, 3200 samples / 11144
loss=2.115, 4000 samples / 11144
loss=2.149, 4800 samples / 11144
loss=2.467, 5600 samples / 11144
loss=2.261, 6400 samples / 11144
loss=4.112, 7200 samples / 11144
loss=3.230, 8000 samples / 11144
loss=3.225, 8800 samples / 11144
train accuracy = 0.30375090
test accuracy = 0.00888370423546303, test loss = 5.484169

epoch:  6 / 15
loss=1.102, 0 samples / 11144
loss=1.206, 800 samples / 11144
loss=0.937, 1600 samples / 11144
loss=1.876, 2400 samples / 11144
loss=1.663, 3200 samples / 11144
loss=0.751, 4000 samples / 11144
loss=1.729, 4800 samples / 11144
loss=0.762, 5600 samples / 11144
loss=0.745, 6400 samples / 11144
loss=1.083, 7200 samples / 11144
loss=0.537, 8000 samples / 11144
loss=2.190, 8800 samples / 11144
train accuracy = 0.62508973
test accuracy = 0.009152907394113424, test loss = 6.557731

epoch:  7 / 15
loss=0.265, 0 samples / 11144
loss=1.248, 800 samples / 11144
loss=0.267, 1600 samples / 11144
loss=0.033, 2400 samples / 11144
loss=0.108, 3200 samples / 11144
loss=0.113, 4000 samples / 11144
loss=0.290, 4800 samples / 11144
loss=0.111, 5600 samples / 11144
loss=0.036, 6400 samples / 11144
loss=0.053, 7200 samples / 11144
loss=0.635, 8000 samples / 11144
loss=0.155, 8800 samples / 11144
train accuracy = 0.76444724
test accuracy = 0.009511844938980617, test loss = 7.408891

epoch:  8 / 15
loss=0.012, 0 samples / 11144
loss=0.021, 800 samples / 11144
loss=0.042, 1600 samples / 11144
loss=0.010, 2400 samples / 11144
loss=0.004, 3200 samples / 11144
loss=0.021, 4000 samples / 11144
loss=0.066, 4800 samples / 11144
loss=0.018, 5600 samples / 11144
loss=0.093, 6400 samples / 11144
loss=0.153, 7200 samples / 11144
loss=0.116, 8000 samples / 11144
loss=2.977, 8800 samples / 11144
train accuracy = 0.78257358
test accuracy = 0.010050251256281407, test loss = 7.421178

epoch:  9 / 15
loss=0.276, 0 samples / 11144
loss=0.021, 800 samples / 11144
loss=0.004, 1600 samples / 11144
loss=0.082, 2400 samples / 11144
loss=0.002, 3200 samples / 11144
loss=0.002, 4000 samples / 11144
loss=0.044, 4800 samples / 11144
loss=0.012, 5600 samples / 11144
loss=0.001, 6400 samples / 11144
loss=0.011, 7200 samples / 11144
loss=0.368, 8000 samples / 11144
loss=0.054, 8800 samples / 11144
train accuracy = 0.78589375
test accuracy = 0.008614501076812634, test loss = 7.721355

epoch:  10 / 15
loss=0.061, 0 samples / 11144
loss=0.152, 800 samples / 11144
loss=0.006, 1600 samples / 11144
loss=0.023, 2400 samples / 11144
loss=0.003, 3200 samples / 11144
loss=0.079, 4000 samples / 11144
loss=0.001, 4800 samples / 11144
loss=0.011, 5600 samples / 11144
loss=0.953, 6400 samples / 11144
loss=0.016, 7200 samples / 11144
loss=0.642, 8000 samples / 11144
loss=0.016, 8800 samples / 11144
train accuracy = 0.78831658
test accuracy = 0.00834529791816224, test loss = 7.896915

epoch:  11 / 15
loss=0.058, 0 samples / 11144
loss=0.017, 800 samples / 11144
loss=0.002, 1600 samples / 11144
loss=0.068, 2400 samples / 11144
loss=0.792, 3200 samples / 11144
loss=0.071, 4000 samples / 11144
loss=0.097, 4800 samples / 11144
loss=0.043, 5600 samples / 11144
loss=0.084, 6400 samples / 11144
loss=0.036, 7200 samples / 11144
loss=0.021, 8000 samples / 11144
loss=0.010, 8800 samples / 11144
train accuracy = 0.78966260
test accuracy = 0.008973438621679828, test loss = 8.180868

epoch:  12 / 15
loss=0.000, 0 samples / 11144
loss=0.003, 800 samples / 11144
loss=0.049, 1600 samples / 11144
loss=0.011, 2400 samples / 11144
loss=0.001, 3200 samples / 11144
loss=0.108, 4000 samples / 11144
loss=0.009, 4800 samples / 11144
loss=0.020, 5600 samples / 11144
loss=0.017, 6400 samples / 11144
loss=0.114, 7200 samples / 11144
loss=0.071, 8000 samples / 11144
loss=0.004, 8800 samples / 11144
train accuracy = 0.79136755
test accuracy = 0.008165829145728644, test loss = 8.219854

epoch:  13 / 15
loss=0.000, 0 samples / 11144
loss=0.008, 800 samples / 11144
loss=0.022, 1600 samples / 11144
loss=0.001, 2400 samples / 11144
loss=0.007, 3200 samples / 11144
loss=0.007, 4000 samples / 11144
loss=0.310, 4800 samples / 11144
loss=0.006, 5600 samples / 11144
loss=0.007, 6400 samples / 11144
loss=0.029, 7200 samples / 11144
loss=0.001, 8000 samples / 11144
loss=0.020, 8800 samples / 11144
train accuracy = 0.79145729
test accuracy = 0.007896625987078248, test loss = 8.050210

epoch:  14 / 15
loss=0.002, 0 samples / 11144
loss=0.061, 800 samples / 11144
loss=0.002, 1600 samples / 11144
loss=0.057, 2400 samples / 11144
loss=0.000, 3200 samples / 11144
loss=0.018, 4000 samples / 11144
loss=0.010, 4800 samples / 11144
loss=0.018, 5600 samples / 11144
loss=0.006, 6400 samples / 11144
loss=0.008, 7200 samples / 11144
loss=0.002, 8000 samples / 11144
loss=0.001, 8800 samples / 11144
train accuracy = 0.79244436
test accuracy = 0.008614501076812634, test loss = 8.275542

epoch:  15 / 15
loss=0.002, 0 samples / 11144
loss=0.000, 800 samples / 11144
loss=0.004, 1600 samples / 11144
loss=0.005, 2400 samples / 11144
loss=0.006, 3200 samples / 11144
loss=0.006, 4000 samples / 11144
loss=0.007, 4800 samples / 11144
loss=0.033, 5600 samples / 11144
loss=0.001, 6400 samples / 11144
loss=0.001, 7200 samples / 11144
loss=0.027, 8000 samples / 11144
loss=0.006, 8800 samples / 11144
train accuracy = 0.79495693
test accuracy = 0.00834529791816224, test loss = 7.813670

fold number: 5
epoch:  1 / 15
loss=5.249, 0 samples / 11144
loss=5.308, 800 samples / 11144
loss=5.286, 1600 samples / 11144
loss=5.310, 2400 samples / 11144
loss=5.347, 3200 samples / 11144
loss=5.254, 4000 samples / 11144
loss=5.272, 4800 samples / 11144
loss=5.256, 5600 samples / 11144
loss=5.406, 6400 samples / 11144
loss=5.253, 7200 samples / 11144
loss=5.401, 8000 samples / 11144
loss=5.229, 8800 samples / 11144
train accuracy = 0.00601220
test accuracy = 0.0008973438621679827, test loss = 5.285375

epoch:  2 / 15
loss=5.293, 0 samples / 11144
loss=5.256, 800 samples / 11144
loss=5.325, 1600 samples / 11144
loss=5.092, 2400 samples / 11144
loss=5.410, 3200 samples / 11144
loss=4.635, 4000 samples / 11144
loss=5.104, 4800 samples / 11144
loss=5.186, 5600 samples / 11144
loss=5.129, 6400 samples / 11144
loss=5.252, 7200 samples / 11144
loss=5.131, 8000 samples / 11144
loss=5.216, 8800 samples / 11144
train accuracy = 0.01328069
test accuracy = 0.0008076094759511845, test loss = 5.298677

epoch:  3 / 15
loss=5.133, 0 samples / 11144
loss=5.064, 800 samples / 11144
loss=4.922, 1600 samples / 11144
loss=4.470, 2400 samples / 11144
loss=4.850, 3200 samples / 11144
loss=4.637, 4000 samples / 11144
loss=4.986, 4800 samples / 11144
loss=4.923, 5600 samples / 11144
loss=4.917, 6400 samples / 11144
loss=5.159, 7200 samples / 11144
loss=4.725, 8000 samples / 11144
loss=5.088, 8800 samples / 11144
train accuracy = 0.03194544
test accuracy = 0.0008076094759511845, test loss = 5.402282

epoch:  4 / 15
loss=4.652, 0 samples / 11144
loss=4.629, 800 samples / 11144
loss=4.549, 1600 samples / 11144
loss=4.237, 2400 samples / 11144
loss=4.611, 3200 samples / 11144
loss=4.952, 4000 samples / 11144
loss=4.324, 4800 samples / 11144
loss=3.733, 5600 samples / 11144
loss=4.258, 6400 samples / 11144
loss=3.987, 7200 samples / 11144
loss=4.816, 8000 samples / 11144
loss=4.073, 8800 samples / 11144
train accuracy = 0.11387294
test accuracy = 0.000987078248384781, test loss = 5.518791

epoch:  5 / 15
loss=2.564, 0 samples / 11144
loss=3.868, 800 samples / 11144
loss=2.107, 1600 samples / 11144
loss=2.346, 2400 samples / 11144
loss=1.971, 3200 samples / 11144
loss=2.250, 4000 samples / 11144
loss=2.485, 4800 samples / 11144
loss=2.257, 5600 samples / 11144
loss=2.031, 6400 samples / 11144
loss=2.849, 7200 samples / 11144
loss=2.241, 8000 samples / 11144
loss=2.356, 8800 samples / 11144
train accuracy = 0.36279612
test accuracy = 0.0008973438621679827, test loss = 6.349324

epoch:  6 / 15
loss=0.806, 0 samples / 11144
loss=1.085, 800 samples / 11144
loss=0.861, 1600 samples / 11144
loss=1.145, 2400 samples / 11144
loss=0.552, 3200 samples / 11144
loss=0.650, 4000 samples / 11144
loss=0.546, 4800 samples / 11144
loss=0.596, 5600 samples / 11144
loss=0.399, 6400 samples / 11144
loss=0.614, 7200 samples / 11144
loss=0.582, 8000 samples / 11144
loss=1.483, 8800 samples / 11144
train accuracy = 0.63119167
test accuracy = 0.0008076094759511845, test loss = 7.411352

epoch:  7 / 15
loss=0.719, 0 samples / 11144
loss=0.137, 800 samples / 11144
loss=0.476, 1600 samples / 11144
loss=0.312, 2400 samples / 11144
loss=0.496, 3200 samples / 11144
loss=0.788, 4000 samples / 11144
loss=0.154, 4800 samples / 11144
loss=0.161, 5600 samples / 11144
loss=0.018, 6400 samples / 11144
loss=0.918, 7200 samples / 11144
loss=0.381, 8000 samples / 11144
loss=0.078, 8800 samples / 11144
train accuracy = 0.73977028
test accuracy = 0.0008076094759511845, test loss = 8.292376

epoch:  8 / 15
loss=0.055, 0 samples / 11144
loss=0.158, 800 samples / 11144
loss=0.323, 1600 samples / 11144
loss=0.155, 2400 samples / 11144
loss=0.240, 3200 samples / 11144
loss=0.033, 4000 samples / 11144
loss=0.167, 4800 samples / 11144
loss=0.147, 5600 samples / 11144
loss=0.044, 6400 samples / 11144
loss=0.320, 7200 samples / 11144
loss=0.178, 8000 samples / 11144
loss=0.074, 8800 samples / 11144
train accuracy = 0.76256281
test accuracy = 0.0007178750897343862, test loss = 8.568236

epoch:  9 / 15
loss=0.040, 0 samples / 11144
loss=0.062, 800 samples / 11144
loss=0.194, 1600 samples / 11144
loss=0.032, 2400 samples / 11144
loss=0.342, 3200 samples / 11144
loss=0.051, 4000 samples / 11144
loss=0.043, 4800 samples / 11144
loss=0.008, 5600 samples / 11144
loss=0.033, 6400 samples / 11144
loss=0.124, 7200 samples / 11144
loss=0.079, 8000 samples / 11144
loss=0.030, 8800 samples / 11144
train accuracy = 0.77099785
test accuracy = 0.0008973438621679827, test loss = 9.303546

epoch:  10 / 15
loss=0.041, 0 samples / 11144
loss=0.017, 800 samples / 11144
loss=0.019, 1600 samples / 11144
loss=0.017, 2400 samples / 11144
loss=0.007, 3200 samples / 11144
loss=0.131, 4000 samples / 11144
loss=0.279, 4800 samples / 11144
loss=0.039, 5600 samples / 11144
loss=0.012, 6400 samples / 11144
loss=0.318, 7200 samples / 11144
loss=0.022, 8000 samples / 11144
loss=0.087, 8800 samples / 11144
train accuracy = 0.77701005
test accuracy = 0.0010768126346015793, test loss = 9.343975

epoch:  11 / 15
loss=0.014, 0 samples / 11144
loss=0.000, 800 samples / 11144
loss=0.052, 1600 samples / 11144
loss=0.034, 2400 samples / 11144
loss=0.004, 3200 samples / 11144
loss=0.050, 4000 samples / 11144
loss=0.193, 4800 samples / 11144
loss=0.029, 5600 samples / 11144
loss=0.008, 6400 samples / 11144
loss=0.078, 7200 samples / 11144
loss=0.013, 8000 samples / 11144
loss=1.592, 8800 samples / 11144
train accuracy = 0.78104810
test accuracy = 0.0008973438621679827, test loss = 8.549863

epoch:  12 / 15
loss=0.018, 0 samples / 11144
loss=0.001, 800 samples / 11144
loss=0.012, 1600 samples / 11144
loss=0.466, 2400 samples / 11144
loss=0.001, 3200 samples / 11144
loss=0.029, 4000 samples / 11144
loss=0.009, 4800 samples / 11144
loss=0.009, 5600 samples / 11144
loss=0.249, 6400 samples / 11144
loss=0.001, 7200 samples / 11144
loss=0.008, 8000 samples / 11144
loss=0.068, 8800 samples / 11144
train accuracy = 0.78257358
test accuracy = 0.0010768126346015793, test loss = 9.146504

epoch:  13 / 15
loss=0.015, 0 samples / 11144
loss=0.308, 800 samples / 11144
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="58" id="UYLc2EUYaL4K">
<div class="sourceCode" id="cb73"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> get_mean_history(model_history, k_folds)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="59" id="7bhg-fmgaO8i">
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>plot_accuracy(epochs, history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/956be6aec5c49f22bb7064da8f94a58e0c8ab9ca.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="62" id="flTmWcQ3aRu9">
<div class="sourceCode" id="cb75"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>plot_loss(epochs, history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/3c919fcf1bd5f5852762854d80b20461afddb5ed.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="61" id="fxQrUKDWaUsL">
<div class="sourceCode" id="cb76"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>history_dataframe <span class="op">=</span> pd.DataFrame(history)</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>history_dataframe</span></code></pre></div>
<div class="output execute_result" data-execution_count="61">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train_loss</th>
      <th>val_loss</th>
      <th>train_acc</th>
      <th>val_acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.2676</td>
      <td>5.278194</td>
      <td>0.004325</td>
      <td>0.001076</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5.2400</td>
      <td>5.271002</td>
      <td>0.006443</td>
      <td>0.001597</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.1874</td>
      <td>5.261649</td>
      <td>0.013370</td>
      <td>0.002548</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.8272</td>
      <td>5.227630</td>
      <td>0.045980</td>
      <td>0.004504</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4.2960</td>
      <td>5.436451</td>
      <td>0.164070</td>
      <td>0.006048</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2.6284</td>
      <td>6.108027</td>
      <td>0.380887</td>
      <td>0.007340</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1.0392</td>
      <td>7.076733</td>
      <td>0.606640</td>
      <td>0.007842</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.8030</td>
      <td>7.729330</td>
      <td>0.745890</td>
      <td>0.007106</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.2474</td>
      <td>8.321421</td>
      <td>0.775162</td>
      <td>0.007160</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.1464</td>
      <td>8.495086</td>
      <td>0.781299</td>
      <td>0.006945</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.0396</td>
      <td>8.665785</td>
      <td>0.784817</td>
      <td>0.006730</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.0376</td>
      <td>8.691509</td>
      <td>0.786953</td>
      <td>0.007160</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.1684</td>
      <td>8.699628</td>
      <td>0.790264</td>
      <td>0.007191</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.0250</td>
      <td>8.791138</td>
      <td>0.783823</td>
      <td>0.006944</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.0136</td>
      <td>8.782570</td>
      <td>0.793412</td>
      <td>0.005809</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell markdown" id="Orka0uiDpl4p">
<p>From the results we gather that the addition of augmentations did not assist us in the improvement of our model.</p>
</div>
<div class="cell markdown" id="l-eW6g1RpxyX">
<p>We can try the following suggestions to improve our model:</p>
<ol>
<li><p>We can try and to train our model on the unmodified data (e.g. without cropping).</p></li>
<li><p>We can try and add the original uncropped images to the training set and train our model on twice the size of samples.</p></li>
<li><p>We can try and swap out our flattening and max pooling with an adaptive average pooling.</p></li>
</ol>
</div>
<section id="the-final-model" class="cell markdown" id="SRji6RJ7SfbB">
<h2>The Final Model</h2>
</section>
<div class="cell markdown" id="FkxLIkzASqxf">
<p>As the second architecture yielded the best results, we will use it as our final model architecture as well and measure its full metrics.</p>
</div>
<section id="defining-the-architecture" class="cell markdown" id="8ZZcHiVSSkOg">
<h3>Defining the architecture</h3>
</section>
<div class="cell code" data-execution_count="31" data-colab="{&quot;height&quot;:0,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;error&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:431,&quot;timestamp&quot;:1670963472177}" id="3uaZoHI9_564" data-outputId="dd71776f-5377-429c-ca61-a7eafe4748a7">
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ConvNetV2(channels, height, width, <span class="dv">3</span>).to(device)</span></code></pre></div>
</div>
<div class="cell markdown" id="nIeAlIm5hTyI">
<p>Setting Model Train Function</p>
</div>
<div class="cell code" data-execution_count="30" data-executionInfo="{&quot;status&quot;:&quot;aborted&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;02267564373530218527&quot;,&quot;displayName&quot;:&quot;Elad Inbar&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:7,&quot;timestamp&quot;:1670963472178}" id="fnaLZW2ATKzM">
<div class="sourceCode" id="cb78"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_and_test(train_dataset, test_dataset, epochs, batch_size, learning_rate, loss_fn):</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>  optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>  history <span class="op">=</span> []</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>  train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>  test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;epoch: &quot;</span>, epoch<span class="op">+</span><span class="dv">1</span>, <span class="st">&quot;/&quot;</span>, epochs)</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>    train_loss, train_acc <span class="op">=</span> train_loop(train_loader, model, device, loss_fn, optimizer)</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>    test_loss, test_acc <span class="op">=</span> test_loop(test_loader, model, device, loss_fn)</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>    history.append({<span class="st">&#39;train_loss&#39;</span>:train_loss.detach().cpu().numpy(), <span class="st">&#39;val_loss&#39;</span>:test_loss, <span class="st">&#39;train_acc&#39;</span>:train_acc, <span class="st">&#39;val_acc&#39;</span>:test_acc})</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>  test_loss, test_acc <span class="op">=</span> test_loop(test_loader, model, device, loss_fn)</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> history</span></code></pre></div>
</div>
<section id="results" class="cell markdown" id="kPz_KEeejraV">
<h3>Results</h3>
</section>
<div class="cell code" data-execution_count="31" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="PAGX6cI3jn1y" data-outputId="da764c56-8941-449f-e4c3-94dd09f632d2">
<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> train_and_test(cropped_cars_train_dataset, cropped_cars_test_dataset, epochs, batch_size, learning_rate, loss_fn)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>epoch:  1 / 15
loss=5.291, 0 samples / 8144
loss=5.282, 800 samples / 8144
loss=5.285, 1600 samples / 8144
loss=5.272, 2400 samples / 8144
loss=5.295, 3200 samples / 8144
loss=5.256, 4000 samples / 8144
loss=5.326, 4800 samples / 8144
loss=5.310, 5600 samples / 8144
loss=5.153, 6400 samples / 8144
loss=5.158, 7200 samples / 8144
loss=5.055, 8000 samples / 8144
train accuracy = 0.00577112
test accuracy = 0.010073373958462879, test loss = 5.207495

epoch:  2 / 15
loss=5.086, 0 samples / 8144
loss=5.303, 800 samples / 8144
loss=5.271, 1600 samples / 8144
loss=4.984, 2400 samples / 8144
loss=5.209, 3200 samples / 8144
loss=4.949, 4000 samples / 8144
loss=5.021, 4800 samples / 8144
loss=5.324, 5600 samples / 8144
loss=5.234, 6400 samples / 8144
loss=5.021, 7200 samples / 8144
loss=5.183, 8000 samples / 8144
train accuracy = 0.01583988
test accuracy = 0.02499689093396344, test loss = 5.094952

epoch:  3 / 15
loss=5.023, 0 samples / 8144
loss=5.033, 800 samples / 8144
loss=4.758, 1600 samples / 8144
loss=5.006, 2400 samples / 8144
loss=5.389, 3200 samples / 8144
loss=4.979, 4000 samples / 8144
loss=4.875, 4800 samples / 8144
loss=5.034, 5600 samples / 8144
loss=5.452, 6400 samples / 8144
loss=4.187, 7200 samples / 8144
loss=4.854, 8000 samples / 8144
train accuracy = 0.03867878
test accuracy = 0.049496331302076854, test loss = 4.891859

epoch:  4 / 15
loss=4.682, 0 samples / 8144
loss=4.607, 800 samples / 8144
loss=3.924, 1600 samples / 8144
loss=4.718, 2400 samples / 8144
loss=4.912, 3200 samples / 8144
loss=4.536, 4000 samples / 8144
loss=4.304, 4800 samples / 8144
loss=4.604, 5600 samples / 8144
loss=4.436, 6400 samples / 8144
loss=3.783, 7200 samples / 8144
loss=4.642, 8000 samples / 8144
train accuracy = 0.10609037
test accuracy = 0.08096007959209053, test loss = 4.630991

epoch:  5 / 15
loss=3.784, 0 samples / 8144
loss=3.547, 800 samples / 8144
loss=3.971, 1600 samples / 8144
loss=3.200, 2400 samples / 8144
loss=3.315, 3200 samples / 8144
loss=2.899, 4000 samples / 8144
loss=2.718, 4800 samples / 8144
loss=3.283, 5600 samples / 8144
loss=2.525, 6400 samples / 8144
loss=3.510, 7200 samples / 8144
loss=3.408, 8000 samples / 8144
train accuracy = 0.27468075
test accuracy = 0.10583260788459146, test loss = 4.502672

epoch:  6 / 15
loss=1.209, 0 samples / 8144
loss=2.770, 800 samples / 8144
loss=2.777, 1600 samples / 8144
loss=2.882, 2400 samples / 8144
loss=1.122, 3200 samples / 8144
loss=2.869, 4000 samples / 8144
loss=2.502, 4800 samples / 8144
loss=1.329, 5600 samples / 8144
loss=1.185, 6400 samples / 8144
loss=1.304, 7200 samples / 8144
loss=1.930, 8000 samples / 8144
train accuracy = 0.53597741
test accuracy = 0.11304564108941674, test loss = 4.743171

epoch:  7 / 15
loss=0.562, 0 samples / 8144
loss=0.536, 800 samples / 8144
loss=1.210, 1600 samples / 8144
loss=0.708, 2400 samples / 8144
loss=1.334, 3200 samples / 8144
loss=0.712, 4000 samples / 8144
loss=0.691, 4800 samples / 8144
loss=0.329, 5600 samples / 8144
loss=1.349, 6400 samples / 8144
loss=0.622, 7200 samples / 8144
loss=0.641, 8000 samples / 8144
train accuracy = 0.77161100
test accuracy = 0.11752269618206691, test loss = 5.134113

epoch:  8 / 15
loss=0.409, 0 samples / 8144
loss=0.559, 800 samples / 8144
loss=0.059, 1600 samples / 8144
loss=1.029, 2400 samples / 8144
loss=1.012, 3200 samples / 8144
loss=0.333, 4000 samples / 8144
loss=0.576, 4800 samples / 8144
loss=2.666, 5600 samples / 8144
loss=0.137, 6400 samples / 8144
loss=0.203, 7200 samples / 8144
loss=0.421, 8000 samples / 8144
train accuracy = 0.89464637
test accuracy = 0.11814450938937943, test loss = 5.422966

epoch:  9 / 15
loss=0.271, 0 samples / 8144
loss=0.439, 800 samples / 8144
loss=0.301, 1600 samples / 8144
loss=0.048, 2400 samples / 8144
loss=0.100, 3200 samples / 8144
loss=0.105, 4000 samples / 8144
loss=0.236, 4800 samples / 8144
loss=0.662, 5600 samples / 8144
loss=0.946, 6400 samples / 8144
loss=0.791, 7200 samples / 8144
loss=0.026, 8000 samples / 8144
train accuracy = 0.93197446
test accuracy = 0.11627906976744186, test loss = 5.593611

epoch:  10 / 15
loss=0.144, 0 samples / 8144
loss=0.339, 800 samples / 8144
loss=0.099, 1600 samples / 8144
loss=0.121, 2400 samples / 8144
loss=0.185, 3200 samples / 8144
loss=0.146, 4000 samples / 8144
loss=0.051, 4800 samples / 8144
loss=0.405, 5600 samples / 8144
loss=0.190, 6400 samples / 8144
loss=0.024, 7200 samples / 8144
loss=0.025, 8000 samples / 8144
train accuracy = 0.95677800
test accuracy = 0.11938813580400448, test loss = 5.852866

epoch:  11 / 15
loss=0.047, 0 samples / 8144
loss=0.030, 800 samples / 8144
loss=0.098, 1600 samples / 8144
loss=0.042, 2400 samples / 8144
loss=0.290, 3200 samples / 8144
loss=0.151, 4000 samples / 8144
loss=0.081, 4800 samples / 8144
loss=0.036, 5600 samples / 8144
loss=0.185, 6400 samples / 8144
loss=0.245, 7200 samples / 8144
loss=0.013, 8000 samples / 8144
train accuracy = 0.96623281
test accuracy = 0.11366745429672927, test loss = 6.063913

epoch:  12 / 15
loss=0.044, 0 samples / 8144
loss=0.009, 800 samples / 8144
loss=0.712, 1600 samples / 8144
loss=0.016, 2400 samples / 8144
loss=0.003, 3200 samples / 8144
loss=0.062, 4000 samples / 8144
loss=0.037, 4800 samples / 8144
loss=0.021, 5600 samples / 8144
loss=0.002, 6400 samples / 8144
loss=0.268, 7200 samples / 8144
loss=0.043, 8000 samples / 8144
train accuracy = 0.97200393
test accuracy = 0.12038303693570451, test loss = 5.993640

epoch:  13 / 15
loss=0.006, 0 samples / 8144
loss=0.003, 800 samples / 8144
loss=0.026, 1600 samples / 8144
loss=0.016, 2400 samples / 8144
loss=0.103, 3200 samples / 8144
loss=0.022, 4000 samples / 8144
loss=0.056, 4800 samples / 8144
loss=0.012, 5600 samples / 8144
loss=0.034, 6400 samples / 8144
loss=0.020, 7200 samples / 8144
loss=0.251, 8000 samples / 8144
train accuracy = 0.97716110
test accuracy = 0.11428926750404178, test loss = 6.058581

epoch:  14 / 15
loss=0.018, 0 samples / 8144
loss=0.012, 800 samples / 8144
loss=0.003, 1600 samples / 8144
loss=0.000, 2400 samples / 8144
loss=0.397, 3200 samples / 8144
loss=0.001, 4000 samples / 8144
loss=0.063, 4800 samples / 8144
loss=0.399, 5600 samples / 8144
loss=0.012, 6400 samples / 8144
loss=0.071, 7200 samples / 8144
loss=0.007, 8000 samples / 8144
train accuracy = 0.97445972
test accuracy = 0.11814450938937943, test loss = 6.085626

epoch:  15 / 15
loss=0.488, 0 samples / 8144
loss=0.003, 800 samples / 8144
loss=0.026, 1600 samples / 8144
loss=0.010, 2400 samples / 8144
loss=0.034, 3200 samples / 8144
loss=0.037, 4000 samples / 8144
loss=0.016, 4800 samples / 8144
loss=0.036, 5600 samples / 8144
loss=0.002, 6400 samples / 8144
loss=0.016, 7200 samples / 8144
loss=0.107, 8000 samples / 8144
train accuracy = 0.97888016
test accuracy = 0.11030966297724164, test loss = 6.111105

test accuracy = 0.11030966297724164, test loss = 6.107167
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="32" id="O_wq4MFyjyt5">
<div class="sourceCode" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>plot_accuracy(epochs, history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/d15256b50d44595d2d0b4b096bc9fdd4bbbed6ff.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="33" id="9ijHoC6RjzAq">
<div class="sourceCode" id="cb82"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>plot_loss(epochs, history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/f4487f7c81075163c3ce9c1530e83c4af8a728f3.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="34" id="7MajsgV8j21t">
<div class="sourceCode" id="cb83"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>history_dataframe <span class="op">=</span> pd.DataFrame(history)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>history_dataframe</span></code></pre></div>
<div class="output execute_result" data-execution_count="34">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train_loss</th>
      <th>val_loss</th>
      <th>train_acc</th>
      <th>val_acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.274221</td>
      <td>5.207495</td>
      <td>0.005771</td>
      <td>0.010073</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5.1697984</td>
      <td>5.094952</td>
      <td>0.015840</td>
      <td>0.024997</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.9502606</td>
      <td>4.891859</td>
      <td>0.038679</td>
      <td>0.049496</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.402196</td>
      <td>4.630991</td>
      <td>0.106090</td>
      <td>0.080960</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.2900712</td>
      <td>4.502672</td>
      <td>0.274681</td>
      <td>0.105833</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1.9172872</td>
      <td>4.743171</td>
      <td>0.535977</td>
      <td>0.113046</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.8907333</td>
      <td>5.134113</td>
      <td>0.771611</td>
      <td>0.117523</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.4258316</td>
      <td>5.422966</td>
      <td>0.894646</td>
      <td>0.118145</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.2723634</td>
      <td>5.593611</td>
      <td>0.931974</td>
      <td>0.116279</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.17319192</td>
      <td>5.852866</td>
      <td>0.956778</td>
      <td>0.119388</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.15432712</td>
      <td>6.063913</td>
      <td>0.966233</td>
      <td>0.113667</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.13367628</td>
      <td>5.993640</td>
      <td>0.972004</td>
      <td>0.120383</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.108916834</td>
      <td>6.058581</td>
      <td>0.977161</td>
      <td>0.114289</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.110247776</td>
      <td>6.085626</td>
      <td>0.974460</td>
      <td>0.118145</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.08625688</td>
      <td>6.111105</td>
      <td>0.978880</td>
      <td>0.110310</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="adding-inference-time-augmentation" class="cell markdown" id="J8hBCpGfnIfj">
<h1><strong>Adding Inference-Time-Augmentation</strong></h1>
</section>
<div class="cell markdown" id="-2hZauR1tIOY">
<p>We will save the augmented samples along with their original versions in a different folder so we can alternate between datasets in case we are not interested in using the augmented dataset.</p>
</div>
<div class="cell code" data-execution_count="35" id="eVGSxOHytFcL">
<div class="sourceCode" id="cb84"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>resize_images(test_path, test_annos_path, processed_test_path)</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>processed_cars_test_dataset <span class="op">=</span> cars_dataset(processed_test_path, y_test_labels_path)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="36">
<div class="sourceCode" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> aug_generator(sample_img):</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>  sample_img <span class="op">=</span> sample_img.numpy()</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>  sample_img <span class="op">=</span> sample_img.transpose()</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>  sample_img <span class="op">=</span> sample_img.astype(np.uint8)</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>  transforms <span class="op">=</span> [</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>      iaa.Fliplr(<span class="fl">0.7</span>), iaa.Affine(rotate<span class="op">=</span><span class="dv">10</span>), iaa.CropAndPad(percent<span class="op">=</span>(<span class="op">-</span><span class="fl">0.25</span>, <span class="fl">0.25</span>))</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>  ]</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>  transformed <span class="op">=</span> []</span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> transform <span class="kw">in</span> transforms:</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>    transformed_image <span class="op">=</span> transform(image<span class="op">=</span>sample_img)</span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>    transformed_image <span class="op">=</span> transformed_image.transpose()</span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>    transformed_image <span class="op">=</span> transformed_image.astype(np.float32)</span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a>    transformed_image <span class="op">=</span> torch.from_numpy(transformed_image)</span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>    transformed.append(transformed_image)</span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> transformed</span></code></pre></div>
</div>
<div class="cell markdown">
<p>Before augmentation</p>
</div>
<div class="cell code" data-execution_count="38">
<div class="sourceCode" id="cb86"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>sample_img, label <span class="op">=</span> processed_cars_test_dataset.<span class="fu">__getitem__</span>(<span class="dv">0</span>)</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>sample_img <span class="op">=</span> sample_img.transpose()</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>sample_img <span class="op">=</span> sample_img.astype(np.uint8)</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1</span>)</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(sample_img)</span></code></pre></div>
<div class="output execute_result" data-execution_count="38">
<pre><code>&lt;matplotlib.image.AxesImage at 0x7f01df19bc50&gt;</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/16b6c3b6759713b3b69cc1ff8b000627ff207599.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>After augmentation</p>
</div>
<div class="cell code" data-execution_count="40">
<div class="sourceCode" id="cb88"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>im, l <span class="op">=</span> processed_cars_test_dataset.<span class="fu">__getitem__</span>(<span class="dv">0</span>)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> torch.from_numpy(im)</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>generated <span class="op">=</span> aug_generator(im)</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx,transformed_image <span class="kw">in</span> <span class="bu">enumerate</span>(generated):</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>  transformed_image <span class="op">=</span> transformed_image.numpy()</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>  transformed_image <span class="op">=</span> transformed_image.transpose()</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>  transformed_image <span class="op">=</span> transformed_image.astype(np.uint8)</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>  plt.subplot(<span class="dv">3</span>, <span class="dv">3</span>, (<span class="dv">1</span><span class="op">+</span>idx))</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>  plt.imshow(transformed_image)</span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/07c1743e62df9aa7032af270510673a3803f2664.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>redefining test loop so each image will be augmentated</p>
</div>
<div class="cell code" data-execution_count="41">
<div class="sourceCode" id="cb89"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_img_TTA_mean(im,model):</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>  mean_results <span class="op">=</span> [<span class="dv">0</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">196</span>)]</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>  aug_imgs <span class="op">=</span> aug_generator(im)</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>  imgs <span class="op">=</span> []</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> img <span class="kw">in</span> aug_imgs:</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>    imgs.append(img)</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> idx , image <span class="kw">in</span> <span class="bu">enumerate</span>(imgs):</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> image.to(device)</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> model(image)[<span class="dv">0</span>]</span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> idx <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>      mean_pred <span class="op">=</span> preds</span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> label <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(preds)):</span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a>        mean_pred[label] <span class="op">+=</span> preds[label]</span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(mean_pred)):</span>
<span id="cb89-21"><a href="#cb89-21" aria-hidden="true" tabindex="-1"></a>    mean_pred[i] <span class="op">/=</span> <span class="bu">len</span>(imgs)</span>
<span id="cb89-22"><a href="#cb89-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-23"><a href="#cb89-23" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> mean_pred.argmax(<span class="dv">0</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="42">
<div class="sourceCode" id="cb90"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_loop_TTA(data_loader, model, device, loss_fn):</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span><span class="bu">len</span>(data_loader.dataset)</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>    test_loss<span class="op">=</span><span class="dv">0</span></span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>    true_positives<span class="op">=</span><span class="dv">0</span></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X,y <span class="kw">in</span> data_loader:</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y.to(dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> idx,im <span class="kw">in</span> <span class="bu">enumerate</span>(X):</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>              pred <span class="op">=</span> get_img_TTA_mean(im,model)</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>              <span class="cf">if</span> y[idx] <span class="op">==</span> pred:</span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>                true_positives <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a>    test_acc <span class="op">=</span> true_positives<span class="op">/</span>size</span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&#39;test accuracy = </span><span class="sc">{</span>test_acc<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> test_loss, test_acc</span></code></pre></div>
</div>
<section id="results" class="cell markdown" id="vg0BMdbxoWaP">
<h2>Results</h2>
</section>
<div class="cell code" data-execution_count="43">
<div class="sourceCode" id="cb91"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>processed_cars_test_dataset <span class="op">=</span> cars_dataset(processed_test_path, y_test_labels_path)</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(processed_cars_test_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> test_loop_TTA(test_loader, model, device, loss_fn)</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>test_res <span class="op">=</span> [{<span class="st">&#39;test_loss&#39;</span>:test_loss, <span class="st">&#39;test_acc&#39;</span>:test_acc}]</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>test_res_dataframe <span class="op">=</span> pd.DataFrame(test_res)</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>test_res_dataframe</span></code></pre></div>
<div class="output stream stdout">
<pre><code>test accuracy = 0.0747419475189653
</code></pre>
</div>
<div class="output execute_result" data-execution_count="43">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>test_loss</th>
      <th>test_acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.074742</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="adding-a-new-category" class="cell markdown" id="wj4-v-gojrKZ">
<h1><strong>Adding a new category</strong></h1>
</section>
<div class="cell markdown" id="EYZaGJUoj1vD">
<p>Adding 'Tesla Model 3' to the dataset</p>
</div>
<div class="cell code" data-execution_count="44" id="GS-F-QI0jom6">
<div class="sourceCode" id="cb93"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>resize_images(extra_train_path, train_annos_path, cropped_train_path, crop<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>resize_images(extra_test_path, test_annos_path, cropped_test_path, crop<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<div class="cell markdown" id="iD1N-KR2muvz">
<p>Redefining the model to reset existing weights</p>
</div>
<div class="cell code" data-execution_count="30" id="CU3ztweVmdjI">
<div class="sourceCode" id="cb94"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ConvNetV2(channels, height, width, <span class="dv">3</span>).to(device)</span></code></pre></div>
</div>
<div class="cell markdown">
<p>We have to modify the final layer in our model to fit the output to the new amount of classes (196-&gt;197).</p>
</div>
<div class="cell code" data-execution_count="31">
<div class="sourceCode" id="cb95"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>model.fc2 <span class="op">=</span> nn.Linear(<span class="dv">1000</span>, <span class="dv">197</span>).to(device)</span></code></pre></div>
</div>
<section id="results" class="cell markdown" id="R8NHUt9emeRD">
<h2>Results</h2>
</section>
<div class="cell code" data-execution_count="32" id="851U57kamhA2">
<div class="sourceCode" id="cb96"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> train_and_test(cropped_cars_train_dataset, cropped_cars_test_dataset, epochs, batch_size, learning_rate, loss_fn)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>epoch:  1 / 15
loss=5.251, 0 samples / 8144
loss=5.218, 800 samples / 8144
loss=5.263, 1600 samples / 8144
loss=5.245, 2400 samples / 8144
loss=5.329, 3200 samples / 8144
loss=5.299, 4000 samples / 8144
loss=5.259, 4800 samples / 8144
loss=5.093, 5600 samples / 8144
loss=5.273, 6400 samples / 8144
loss=5.316, 7200 samples / 8144
loss=5.215, 8000 samples / 8144
train accuracy = 0.00761297
test accuracy = 0.015296604899888074, test loss = 5.217273

epoch:  2 / 15
loss=5.271, 0 samples / 8144
loss=5.382, 800 samples / 8144
loss=5.248, 1600 samples / 8144
loss=5.308, 2400 samples / 8144
loss=5.278, 3200 samples / 8144
loss=5.305, 4000 samples / 8144
loss=4.886, 4800 samples / 8144
loss=5.282, 5600 samples / 8144
loss=5.324, 6400 samples / 8144
loss=5.216, 7200 samples / 8144
loss=5.250, 8000 samples / 8144
train accuracy = 0.01534872
test accuracy = 0.025494341499813455, test loss = 5.087288

epoch:  3 / 15
loss=4.975, 0 samples / 8144
loss=5.077, 800 samples / 8144
loss=4.697, 1600 samples / 8144
loss=4.799, 2400 samples / 8144
loss=5.086, 3200 samples / 8144
loss=4.836, 4000 samples / 8144
loss=4.473, 4800 samples / 8144
loss=5.252, 5600 samples / 8144
loss=4.352, 6400 samples / 8144
loss=4.422, 7200 samples / 8144
loss=4.705, 8000 samples / 8144
train accuracy = 0.03855599
test accuracy = 0.0507399577167019, test loss = 4.877318

epoch:  4 / 15
loss=4.956, 0 samples / 8144
loss=4.932, 800 samples / 8144
loss=4.640, 1600 samples / 8144
loss=4.698, 2400 samples / 8144
loss=3.923, 3200 samples / 8144
loss=4.136, 4000 samples / 8144
loss=5.194, 4800 samples / 8144
loss=4.502, 5600 samples / 8144
loss=4.294, 6400 samples / 8144
loss=3.877, 7200 samples / 8144
loss=3.916, 8000 samples / 8144
train accuracy = 0.10216110
test accuracy = 0.08394478298719064, test loss = 4.611954

epoch:  5 / 15
loss=3.694, 0 samples / 8144
loss=4.318, 800 samples / 8144
loss=3.777, 1600 samples / 8144
loss=3.336, 2400 samples / 8144
loss=3.706, 3200 samples / 8144
loss=3.342, 4000 samples / 8144
loss=3.507, 4800 samples / 8144
loss=3.835, 5600 samples / 8144
loss=3.116, 6400 samples / 8144
loss=3.863, 7200 samples / 8144
loss=3.927, 8000 samples / 8144
train accuracy = 0.25356090
test accuracy = 0.1011068275090163, test loss = 4.528798

epoch:  6 / 15
loss=2.433, 0 samples / 8144
loss=2.192, 800 samples / 8144
loss=2.401, 1600 samples / 8144
loss=2.026, 2400 samples / 8144
loss=2.161, 3200 samples / 8144
loss=1.623, 4000 samples / 8144
loss=3.081, 4800 samples / 8144
loss=1.954, 5600 samples / 8144
loss=3.231, 6400 samples / 8144
loss=2.071, 7200 samples / 8144
loss=1.852, 8000 samples / 8144
train accuracy = 0.49889489
test accuracy = 0.1093147618455416, test loss = 4.739199

epoch:  7 / 15
loss=2.068, 0 samples / 8144
loss=0.344, 800 samples / 8144
loss=0.942, 1600 samples / 8144
loss=1.674, 2400 samples / 8144
loss=0.390, 3200 samples / 8144
loss=1.410, 4000 samples / 8144
loss=0.828, 4800 samples / 8144
loss=0.680, 5600 samples / 8144
loss=1.094, 6400 samples / 8144
loss=0.849, 7200 samples / 8144
loss=2.035, 8000 samples / 8144
train accuracy = 0.73686149
test accuracy = 0.10794677278945405, test loss = 5.058652

epoch:  8 / 15
loss=0.313, 0 samples / 8144
loss=0.233, 800 samples / 8144
loss=0.320, 1600 samples / 8144
loss=0.254, 2400 samples / 8144
loss=0.294, 3200 samples / 8144
loss=0.531, 4000 samples / 8144
loss=0.094, 4800 samples / 8144
loss=0.528, 5600 samples / 8144
loss=0.380, 6400 samples / 8144
loss=0.703, 7200 samples / 8144
loss=0.237, 8000 samples / 8144
train accuracy = 0.87991159
test accuracy = 0.11615470712597936, test loss = 5.507739

epoch:  9 / 15
loss=0.179, 0 samples / 8144
loss=0.145, 800 samples / 8144
loss=0.239, 1600 samples / 8144
loss=0.097, 2400 samples / 8144
loss=0.643, 3200 samples / 8144
loss=0.146, 4000 samples / 8144
loss=0.315, 4800 samples / 8144
loss=0.341, 5600 samples / 8144
loss=0.368, 6400 samples / 8144
loss=0.632, 7200 samples / 8144
loss=0.116, 8000 samples / 8144
train accuracy = 0.92730845
test accuracy = 0.11453799278696679, test loss = 5.588275

epoch:  10 / 15
loss=0.050, 0 samples / 8144
loss=0.089, 800 samples / 8144
loss=0.137, 1600 samples / 8144
loss=0.241, 2400 samples / 8144
loss=0.299, 3200 samples / 8144
loss=0.035, 4000 samples / 8144
loss=0.146, 4800 samples / 8144
loss=0.032, 5600 samples / 8144
loss=0.203, 6400 samples / 8144
loss=0.102, 7200 samples / 8144
loss=0.010, 8000 samples / 8144
train accuracy = 0.94977898
test accuracy = 0.11839323467230443, test loss = 5.806097

epoch:  11 / 15
loss=0.261, 0 samples / 8144
loss=0.077, 800 samples / 8144
loss=0.036, 1600 samples / 8144
loss=0.019, 2400 samples / 8144
loss=0.098, 3200 samples / 8144
loss=0.073, 4000 samples / 8144
loss=0.034, 4800 samples / 8144
loss=0.015, 5600 samples / 8144
loss=0.025, 6400 samples / 8144
loss=0.146, 7200 samples / 8144
loss=0.060, 8000 samples / 8144
train accuracy = 0.96193517
test accuracy = 0.11155328939186668, test loss = 5.654415

epoch:  12 / 15
loss=0.006, 0 samples / 8144
loss=0.045, 800 samples / 8144
loss=0.039, 1600 samples / 8144
loss=0.521, 2400 samples / 8144
loss=0.159, 3200 samples / 8144
loss=0.555, 4000 samples / 8144
loss=0.339, 4800 samples / 8144
loss=0.047, 5600 samples / 8144
loss=0.012, 6400 samples / 8144
loss=0.099, 7200 samples / 8144
loss=0.041, 8000 samples / 8144
train accuracy = 0.96807466
test accuracy = 0.11416490486257928, test loss = 6.140409

epoch:  13 / 15
loss=0.340, 0 samples / 8144
loss=0.015, 800 samples / 8144
loss=0.009, 1600 samples / 8144
loss=0.034, 2400 samples / 8144
loss=0.087, 3200 samples / 8144
loss=0.006, 4000 samples / 8144
loss=0.002, 4800 samples / 8144
loss=0.276, 5600 samples / 8144
loss=0.098, 6400 samples / 8144
loss=0.123, 7200 samples / 8144
loss=0.269, 8000 samples / 8144
train accuracy = 0.97360020
test accuracy = 0.11702524561621688, test loss = 6.154697

epoch:  14 / 15
loss=0.009, 0 samples / 8144
loss=0.046, 800 samples / 8144
loss=0.021, 1600 samples / 8144
loss=0.046, 2400 samples / 8144
loss=0.017, 3200 samples / 8144
loss=0.109, 4000 samples / 8144
loss=0.376, 4800 samples / 8144
loss=0.084, 5600 samples / 8144
loss=0.092, 6400 samples / 8144
loss=0.018, 7200 samples / 8144
loss=0.029, 8000 samples / 8144
train accuracy = 0.97445972
test accuracy = 0.11951249844546698, test loss = 5.994017

epoch:  15 / 15
loss=0.128, 0 samples / 8144
loss=0.013, 800 samples / 8144
loss=0.000, 1600 samples / 8144
loss=0.004, 2400 samples / 8144
loss=0.001, 3200 samples / 8144
loss=0.275, 4000 samples / 8144
loss=0.003, 4800 samples / 8144
loss=0.003, 5600 samples / 8144
loss=0.002, 6400 samples / 8144
loss=0.188, 7200 samples / 8144
loss=0.006, 8000 samples / 8144
train accuracy = 0.97777505
test accuracy = 0.1201343116527795, test loss = 6.271949

test accuracy = 0.1201343116527795, test loss = 6.275988
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="33" id="rMv_IgQZmn34">
<div class="sourceCode" id="cb98"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>plot_accuracy(epochs,history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/046d5ee8e045c599fd24a0e84f133afed44d0c25.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="34" id="KKmufq48moEv">
<div class="sourceCode" id="cb99"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>plot_loss(epochs,history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/0980b0078bf6ab5bd50534b9fa3395707cb5a417.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="35" id="OptfvuuzmqAk">
<div class="sourceCode" id="cb100"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>history_dataframe <span class="op">=</span> pd.DataFrame(history)</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>history_dataframe</span></code></pre></div>
<div class="output execute_result" data-execution_count="35">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train_loss</th>
      <th>val_loss</th>
      <th>train_acc</th>
      <th>val_acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.281345</td>
      <td>5.217273</td>
      <td>0.007613</td>
      <td>0.015297</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5.1727276</td>
      <td>5.087288</td>
      <td>0.015349</td>
      <td>0.025494</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.9377537</td>
      <td>4.877318</td>
      <td>0.038556</td>
      <td>0.050740</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.411532</td>
      <td>4.611954</td>
      <td>0.102161</td>
      <td>0.083945</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.3882606</td>
      <td>4.528798</td>
      <td>0.253561</td>
      <td>0.101107</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2.109445</td>
      <td>4.739199</td>
      <td>0.498895</td>
      <td>0.109315</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1.0454562</td>
      <td>5.058652</td>
      <td>0.736861</td>
      <td>0.107947</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.47362393</td>
      <td>5.507739</td>
      <td>0.879912</td>
      <td>0.116155</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.28844383</td>
      <td>5.588275</td>
      <td>0.927308</td>
      <td>0.114538</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.20497681</td>
      <td>5.806097</td>
      <td>0.949779</td>
      <td>0.118393</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.16807498</td>
      <td>5.654415</td>
      <td>0.961935</td>
      <td>0.111553</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.13177748</td>
      <td>6.140409</td>
      <td>0.968075</td>
      <td>0.114165</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.11617284</td>
      <td>6.154697</td>
      <td>0.973600</td>
      <td>0.117025</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.108823866</td>
      <td>5.994017</td>
      <td>0.974460</td>
      <td>0.119512</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.09365704</td>
      <td>6.271949</td>
      <td>0.977775</td>
      <td>0.120134</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="transfer-learning" class="cell markdown" id="pTxAvk7yLph4">
<h1><strong>Transfer Learning</strong></h1>
</section>
<div class="cell markdown" id="ELzRlZAbLOho">
<p>We chose to use resnet18 as our pretrained model.</p>
</div>
<div class="cell markdown" id="rot0jNk1Lhfq">
<p>Because our pre-trained model already has weigths trained on ImageNet samples, we wouldn't like to change its weights at the beginning. So here we will freeze all the model's gradients except for the last layer we added and train the last layer on its own on our train data set.</p>
</div>
<div class="cell code" data-execution_count="68" id="15zREZBgLRDi">
<div class="sourceCode" id="cb101"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models <span class="im">import</span> resnet18, ResNet18_Weights</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="co"># learning rate</span></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss Function</span></span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of classes in the dataset</span></span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">197</span></span>
<span id="cb101-11"><a href="#cb101-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-12"><a href="#cb101-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Batch size for training (change depending on how much memory you have)</span></span>
<span id="cb101-13"><a href="#cb101-13" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb101-14"><a href="#cb101-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-15"><a href="#cb101-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of epochs to train for</span></span>
<span id="cb101-16"><a href="#cb101-16" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb101-17"><a href="#cb101-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-18"><a href="#cb101-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Flag for feature extracting. When False, we finetune the whole model,</span></span>
<span id="cb101-19"><a href="#cb101-19" aria-hidden="true" tabindex="-1"></a><span class="co">#   when True we only update the reshaped layer params</span></span>
<span id="cb101-20"><a href="#cb101-20" aria-hidden="true" tabindex="-1"></a>feature_extract <span class="op">=</span> <span class="va">True</span></span>
<span id="cb101-21"><a href="#cb101-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-22"><a href="#cb101-22" aria-hidden="true" tabindex="-1"></a>pretrained_model <span class="op">=</span> resnet18()</span>
<span id="cb101-23"><a href="#cb101-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-24"><a href="#cb101-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_parameter_requires_grad(model, feature_extracting):</span>
<span id="cb101-25"><a href="#cb101-25" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> feature_extracting:</span>
<span id="cb101-26"><a href="#cb101-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb101-27"><a href="#cb101-27" aria-hidden="true" tabindex="-1"></a>      param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb101-28"><a href="#cb101-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-29"><a href="#cb101-29" aria-hidden="true" tabindex="-1"></a>set_parameter_requires_grad(pretrained_model, feature_extract)</span>
<span id="cb101-30"><a href="#cb101-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-31"><a href="#cb101-31" aria-hidden="true" tabindex="-1"></a>pretrained_model.fc <span class="op">=</span> nn.Linear(<span class="dv">512</span>, num_classes)</span></code></pre></div>
</div>
<section id="training-fc-layer" class="cell markdown" id="N-sZnMbsLlWG">
<h2>Training FC layer</h2>
</section>
<div class="cell code" data-execution_count="69" id="nKwfKAvPNzBu">
<div class="sourceCode" id="cb102"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>cropped_cars_train_dataset <span class="op">=</span> cars_dataset(cropped_train_path, y_train_labels_path)</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>cropped_cars_test_dataset <span class="op">=</span> cars_dataset(cropped_test_path, y_test_labels_path)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="70" id="TlrF-SZfLVxD">
<div class="sourceCode" id="cb103"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>train_subsampler <span class="op">=</span> torch.utils.data.SubsetRandomSampler([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(cropped_cars_train_dataset.<span class="fu">__len__</span>())])</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(processed_train_path, batch_size, train_subsampler)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="39" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;03923172999087610124&quot;,&quot;displayName&quot;:&quot;colabusage ido&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:2329920,&quot;timestamp&quot;:1671037031954}" id="pQLRidZFLXqT" data-outputId="15512f95-1fa6-452d-84c0-2fe025a744d8">
<div class="sourceCode" id="cb104"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, dataset, epochs, batch_size, learning_rate, loss_fn): </span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>  history <span class="op">=</span> []</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>  train_subsampler <span class="op">=</span> torch.utils.data.SubsetRandomSampler([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dataset.<span class="fu">__len__</span>())])</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>  model.to(device)</span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>  optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a>  train_loader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span>batch_size, sampler<span class="op">=</span> train_subsampler)</span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;epoch: &quot;</span>, epoch<span class="op">+</span><span class="dv">1</span>, <span class="st">&quot;/&quot;</span>, epochs)</span>
<span id="cb104-9"><a href="#cb104-9" aria-hidden="true" tabindex="-1"></a>    train_loss, train_acc <span class="op">=</span> train_loop(train_loader, model, device, loss_fn, optimizer)</span>
<span id="cb104-10"><a href="#cb104-10" aria-hidden="true" tabindex="-1"></a>    history.append({<span class="st">&#39;train_loss&#39;</span>:train_loss.detach().cpu().numpy(), <span class="st">&#39;train_acc&#39;</span>:train_acc})</span>
<span id="cb104-11"><a href="#cb104-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-12"><a href="#cb104-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb104-13"><a href="#cb104-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-14"><a href="#cb104-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> history</span>
<span id="cb104-15"><a href="#cb104-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-16"><a href="#cb104-16" aria-hidden="true" tabindex="-1"></a>train(pretrained_model, cars_train_dataset, epochs<span class="op">=</span>num_epochs, batch_size<span class="op">=</span>batch_size, learning_rate<span class="op">=</span>learning_rate, loss_fn<span class="op">=</span>loss_fn)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>epoch:  1 / 30
loss=5.460, 0 samples / 8144
loss=5.729, 3200 samples / 8144
loss=5.373, 6400 samples / 8144
train accuracy = 0.00834971

epoch:  2 / 30
loss=5.346, 0 samples / 8144
loss=5.544, 3200 samples / 8144
loss=5.435, 6400 samples / 8144
train accuracy = 0.00822692

epoch:  3 / 30
loss=5.155, 0 samples / 8144
loss=5.529, 3200 samples / 8144
loss=5.339, 6400 samples / 8144
train accuracy = 0.01461198

epoch:  4 / 30
loss=5.122, 0 samples / 8144
loss=5.211, 3200 samples / 8144
loss=5.230, 6400 samples / 8144
train accuracy = 0.01596267

epoch:  5 / 30
loss=5.030, 0 samples / 8144
loss=5.203, 3200 samples / 8144
loss=5.280, 6400 samples / 8144
train accuracy = 0.01915521

epoch:  6 / 30
loss=5.284, 0 samples / 8144
loss=5.270, 3200 samples / 8144
loss=4.997, 6400 samples / 8144
train accuracy = 0.02333006

epoch:  7 / 30
loss=4.913, 0 samples / 8144
loss=5.473, 3200 samples / 8144
loss=5.293, 6400 samples / 8144
train accuracy = 0.02357564

epoch:  8 / 30
loss=5.293, 0 samples / 8144
loss=5.063, 3200 samples / 8144
loss=5.216, 6400 samples / 8144
train accuracy = 0.02787328

epoch:  9 / 30
loss=4.942, 0 samples / 8144
loss=4.989, 3200 samples / 8144
loss=5.132, 6400 samples / 8144
train accuracy = 0.03131139

epoch:  10 / 30
loss=4.467, 0 samples / 8144
loss=4.985, 3200 samples / 8144
loss=5.245, 6400 samples / 8144
train accuracy = 0.02910118

epoch:  11 / 30
loss=4.724, 0 samples / 8144
loss=4.986, 3200 samples / 8144
loss=5.031, 6400 samples / 8144
train accuracy = 0.03438114

epoch:  12 / 30
loss=4.982, 0 samples / 8144
loss=4.914, 3200 samples / 8144
loss=4.579, 6400 samples / 8144
train accuracy = 0.04101179

epoch:  13 / 30
loss=5.175, 0 samples / 8144
loss=4.912, 3200 samples / 8144
loss=4.884, 6400 samples / 8144
train accuracy = 0.03855599

epoch:  14 / 30
loss=5.055, 0 samples / 8144
loss=4.898, 3200 samples / 8144
loss=5.053, 6400 samples / 8144
train accuracy = 0.04334479

epoch:  15 / 30
loss=4.852, 0 samples / 8144
loss=4.887, 3200 samples / 8144
loss=4.756, 6400 samples / 8144
train accuracy = 0.04776523

epoch:  16 / 30
loss=4.891, 0 samples / 8144
loss=4.571, 3200 samples / 8144
loss=4.807, 6400 samples / 8144
train accuracy = 0.04715128

epoch:  17 / 30
loss=4.648, 0 samples / 8144
loss=4.959, 3200 samples / 8144
loss=4.715, 6400 samples / 8144
train accuracy = 0.05206287

epoch:  18 / 30
loss=4.561, 0 samples / 8144
loss=5.222, 3200 samples / 8144
loss=4.673, 6400 samples / 8144
train accuracy = 0.05857073

epoch:  19 / 30
loss=4.849, 0 samples / 8144
loss=4.691, 3200 samples / 8144
loss=4.814, 6400 samples / 8144
train accuracy = 0.06004420

epoch:  20 / 30
loss=4.794, 0 samples / 8144
loss=5.041, 3200 samples / 8144
loss=4.694, 6400 samples / 8144
train accuracy = 0.06593811

epoch:  21 / 30
loss=4.375, 0 samples / 8144
loss=4.884, 3200 samples / 8144
loss=4.756, 6400 samples / 8144
train accuracy = 0.06004420

epoch:  22 / 30
loss=4.664, 0 samples / 8144
loss=4.624, 3200 samples / 8144
loss=4.764, 6400 samples / 8144
train accuracy = 0.06900786

epoch:  23 / 30
loss=4.717, 0 samples / 8144
loss=4.370, 3200 samples / 8144
loss=4.656, 6400 samples / 8144
train accuracy = 0.07932220

epoch:  24 / 30
loss=4.440, 0 samples / 8144
loss=4.633, 3200 samples / 8144
loss=4.796, 6400 samples / 8144
train accuracy = 0.07625246

epoch:  25 / 30
loss=4.764, 0 samples / 8144
loss=4.384, 3200 samples / 8144
loss=4.448, 6400 samples / 8144
train accuracy = 0.07870825

epoch:  26 / 30
loss=4.992, 0 samples / 8144
loss=5.071, 3200 samples / 8144
loss=4.614, 6400 samples / 8144
train accuracy = 0.07870825

epoch:  27 / 30
loss=4.428, 0 samples / 8144
loss=4.712, 3200 samples / 8144
loss=4.721, 6400 samples / 8144
train accuracy = 0.08877701

epoch:  28 / 30
loss=4.192, 0 samples / 8144
loss=4.485, 3200 samples / 8144
loss=4.847, 6400 samples / 8144
train accuracy = 0.09098723

epoch:  29 / 30
loss=4.428, 0 samples / 8144
loss=4.691, 3200 samples / 8144
loss=4.286, 6400 samples / 8144
train accuracy = 0.08644401

epoch:  30 / 30
loss=4.561, 0 samples / 8144
loss=4.368, 3200 samples / 8144
loss=4.603, 6400 samples / 8144
train accuracy = 0.08877701

</code></pre>
</div>
<div class="output execute_result" data-execution_count="39">
<pre><code>[{&#39;train_loss&#39;: array(5.493804, dtype=float32),
  &#39;train_acc&#39;: 0.008349705304518664},
 {&#39;train_loss&#39;: array(5.38616, dtype=float32),
  &#39;train_acc&#39;: 0.008226915520628684},
 {&#39;train_loss&#39;: array(5.3356915, dtype=float32),
  &#39;train_acc&#39;: 0.014611984282907662},
 {&#39;train_loss&#39;: array(5.285182, dtype=float32),
  &#39;train_acc&#39;: 0.015962671905697445},
 {&#39;train_loss&#39;: array(5.2533655, dtype=float32),
  &#39;train_acc&#39;: 0.019155206286836934},
 {&#39;train_loss&#39;: array(5.1909685, dtype=float32),
  &#39;train_acc&#39;: 0.023330058939096267},
 {&#39;train_loss&#39;: array(5.15503, dtype=float32),
  &#39;train_acc&#39;: 0.023575638506876228},
 {&#39;train_loss&#39;: array(5.1212425, dtype=float32),
  &#39;train_acc&#39;: 0.02787328094302554},
 {&#39;train_loss&#39;: array(5.078077, dtype=float32),
  &#39;train_acc&#39;: 0.03131139489194499},
 {&#39;train_loss&#39;: array(5.045804, dtype=float32),
  &#39;train_acc&#39;: 0.029101178781925342},
 {&#39;train_loss&#39;: array(5.0184107, dtype=float32),
  &#39;train_acc&#39;: 0.0343811394891945},
 {&#39;train_loss&#39;: array(4.971098, dtype=float32),
  &#39;train_acc&#39;: 0.04101178781925344},
 {&#39;train_loss&#39;: array(4.943593, dtype=float32),
  &#39;train_acc&#39;: 0.03855599214145383},
 {&#39;train_loss&#39;: array(4.9209833, dtype=float32),
  &#39;train_acc&#39;: 0.04334479371316306},
 {&#39;train_loss&#39;: array(4.8947673, dtype=float32),
  &#39;train_acc&#39;: 0.04776522593320236},
 {&#39;train_loss&#39;: array(4.870709, dtype=float32),
  &#39;train_acc&#39;: 0.047151277013752456},
 {&#39;train_loss&#39;: array(4.8351307, dtype=float32),
  &#39;train_acc&#39;: 0.05206286836935167},
 {&#39;train_loss&#39;: array(4.793871, dtype=float32),
  &#39;train_acc&#39;: 0.05857072691552063},
 {&#39;train_loss&#39;: array(4.7699485, dtype=float32),
  &#39;train_acc&#39;: 0.060044204322200395},
 {&#39;train_loss&#39;: array(4.7489805, dtype=float32),
  &#39;train_acc&#39;: 0.06593811394891945},
 {&#39;train_loss&#39;: array(4.72795, dtype=float32),
  &#39;train_acc&#39;: 0.060044204322200395},
 {&#39;train_loss&#39;: array(4.7000356, dtype=float32),
  &#39;train_acc&#39;: 0.06900785854616896},
 {&#39;train_loss&#39;: array(4.660711, dtype=float32),
  &#39;train_acc&#39;: 0.07932220039292731},
 {&#39;train_loss&#39;: array(4.65379, dtype=float32),
  &#39;train_acc&#39;: 0.0762524557956778},
 {&#39;train_loss&#39;: array(4.63073, dtype=float32),
  &#39;train_acc&#39;: 0.0787082514734774},
 {&#39;train_loss&#39;: array(4.605666, dtype=float32),
  &#39;train_acc&#39;: 0.0787082514734774},
 {&#39;train_loss&#39;: array(4.578343, dtype=float32),
  &#39;train_acc&#39;: 0.0887770137524558},
 {&#39;train_loss&#39;: array(4.549584, dtype=float32),
  &#39;train_acc&#39;: 0.09098722986247544},
 {&#39;train_loss&#39;: array(4.5358863, dtype=float32),
  &#39;train_acc&#39;: 0.08644400785854617},
 {&#39;train_loss&#39;: array(4.511991, dtype=float32),
  &#39;train_acc&#39;: 0.0887770137524558}]</code></pre>
</div>
</div>
<section id="test-results" class="cell markdown" id="gUN3vYejLsRu">
<h2>Test Results</h2>
</section>
<div class="cell code" data-execution_count="40" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;03923172999087610124&quot;,&quot;displayName&quot;:&quot;colabusage ido&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:75544,&quot;timestamp&quot;:1671037234443}" id="90hnfByxLobE" data-outputId="666f6ad8-cf23-430f-df58-53d5946949fd">
<div class="sourceCode" id="cb107"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(cars_test_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> test_loop(test_loader, pretrained_model, device, loss_fn)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>test accuracy = 0.026862330555901006, test loss = 5.180982
</code></pre>
</div>
</div>
<section id="fine-tuning" class="cell markdown" id="01HXxFgaLvSo">
<h2>Fine Tuning</h2>
<p>After training our new fully connected layer, we will unfreeze our model's gradients in order to train it for our dataset.</p>
</section>
<div class="cell code" data-execution_count="41" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;03923172999087610124&quot;,&quot;displayName&quot;:&quot;colabusage ido&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:1441723,&quot;timestamp&quot;:1671038726150}" id="ey3zi2I6Ly7p" data-outputId="8ae82baf-aa38-44bb-d66b-4e368c88bf42">
<div class="sourceCode" id="cb109"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">18</span></span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> pretrained_model.parameters():</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>  param.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>train(pretrained_model, cropped_cars_train_dataset, epochs<span class="op">=</span>num_epochs, batch_size<span class="op">=</span>batch_size, learning_rate<span class="op">=</span><span class="fl">0.01</span>, loss_fn<span class="op">=</span>loss_fn)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>epoch:  1 / 18
loss=4.311, 0 samples / 8144
loss=5.307, 3200 samples / 8144
loss=5.164, 6400 samples / 8144
train accuracy = 0.00908644

epoch:  2 / 18
loss=5.230, 0 samples / 8144
loss=5.203, 3200 samples / 8144
loss=4.997, 6400 samples / 8144
train accuracy = 0.01682220

epoch:  3 / 18
loss=5.134, 0 samples / 8144
loss=5.160, 3200 samples / 8144
loss=4.913, 6400 samples / 8144
train accuracy = 0.02480354

epoch:  4 / 18
loss=4.730, 0 samples / 8144
loss=4.775, 3200 samples / 8144
loss=4.984, 6400 samples / 8144
train accuracy = 0.03438114

epoch:  5 / 18
loss=5.138, 0 samples / 8144
loss=4.999, 3200 samples / 8144
loss=4.654, 6400 samples / 8144
train accuracy = 0.04322200

epoch:  6 / 18
loss=4.523, 0 samples / 8144
loss=4.509, 3200 samples / 8144
loss=4.532, 6400 samples / 8144
train accuracy = 0.05832515

epoch:  7 / 18
loss=4.307, 0 samples / 8144
loss=4.414, 3200 samples / 8144
loss=4.125, 6400 samples / 8144
train accuracy = 0.07748035

epoch:  8 / 18
loss=4.617, 0 samples / 8144
loss=4.361, 3200 samples / 8144
loss=3.861, 6400 samples / 8144
train accuracy = 0.10424853

epoch:  9 / 18
loss=3.468, 0 samples / 8144
loss=3.573, 3200 samples / 8144
loss=3.983, 6400 samples / 8144
train accuracy = 0.15238212

epoch:  10 / 18
loss=3.307, 0 samples / 8144
loss=3.310, 3200 samples / 8144
loss=3.171, 6400 samples / 8144
train accuracy = 0.21635560

epoch:  11 / 18
loss=2.565, 0 samples / 8144
loss=2.276, 3200 samples / 8144
loss=2.839, 6400 samples / 8144
train accuracy = 0.30943026

epoch:  12 / 18
loss=2.123, 0 samples / 8144
loss=2.027, 3200 samples / 8144
loss=2.049, 6400 samples / 8144
train accuracy = 0.42472986

epoch:  13 / 18
loss=1.235, 0 samples / 8144
loss=1.460, 3200 samples / 8144
loss=1.532, 6400 samples / 8144
train accuracy = 0.54837917

epoch:  14 / 18
loss=0.909, 0 samples / 8144
loss=0.726, 3200 samples / 8144
loss=1.146, 6400 samples / 8144
train accuracy = 0.70137525

epoch:  15 / 18
loss=0.436, 0 samples / 8144
loss=0.631, 3200 samples / 8144
loss=0.772, 6400 samples / 8144
train accuracy = 0.80169450

epoch:  16 / 18
loss=0.231, 0 samples / 8144
loss=0.265, 3200 samples / 8144
loss=0.499, 6400 samples / 8144
train accuracy = 0.88494597

epoch:  17 / 18
loss=0.423, 0 samples / 8144
loss=0.175, 3200 samples / 8144
loss=0.138, 6400 samples / 8144
train accuracy = 0.94093811

epoch:  18 / 18
loss=0.092, 0 samples / 8144
loss=0.163, 3200 samples / 8144
loss=0.106, 6400 samples / 8144
train accuracy = 0.95960216

</code></pre>
</div>
<div class="output execute_result" data-execution_count="41">
<pre><code>[{&#39;train_loss&#39;: array(5.594163, dtype=float32),
  &#39;train_acc&#39;: 0.009086444007858546},
 {&#39;train_loss&#39;: array(5.115736, dtype=float32),
  &#39;train_acc&#39;: 0.016822200392927308},
 {&#39;train_loss&#39;: array(5.0125527, dtype=float32),
  &#39;train_acc&#39;: 0.02480353634577603},
 {&#39;train_loss&#39;: array(4.909724, dtype=float32),
  &#39;train_acc&#39;: 0.0343811394891945},
 {&#39;train_loss&#39;: array(4.7917786, dtype=float32),
  &#39;train_acc&#39;: 0.043222003929273084},
 {&#39;train_loss&#39;: array(4.608514, dtype=float32),
  &#39;train_acc&#39;: 0.05832514734774067},
 {&#39;train_loss&#39;: array(4.375958, dtype=float32),
  &#39;train_acc&#39;: 0.0774803536345776},
 {&#39;train_loss&#39;: array(4.1099195, dtype=float32),
  &#39;train_acc&#39;: 0.10424852652259332},
 {&#39;train_loss&#39;: array(3.7329197, dtype=float32),
  &#39;train_acc&#39;: 0.15238212180746563},
 {&#39;train_loss&#39;: array(3.2903438, dtype=float32),
  &#39;train_acc&#39;: 0.21635559921414538},
 {&#39;train_loss&#39;: array(2.7349281, dtype=float32),
  &#39;train_acc&#39;: 0.3094302554027505},
 {&#39;train_loss&#39;: array(2.174664, dtype=float32),
  &#39;train_acc&#39;: 0.42472986247544203},
 {&#39;train_loss&#39;: array(1.6333345, dtype=float32),
  &#39;train_acc&#39;: 0.5483791748526523},
 {&#39;train_loss&#39;: array(1.0546144, dtype=float32),
  &#39;train_acc&#39;: 0.7013752455795678},
 {&#39;train_loss&#39;: array(0.67822725, dtype=float32),
  &#39;train_acc&#39;: 0.8016944990176817},
 {&#39;train_loss&#39;: array(0.39590728, dtype=float32),
  &#39;train_acc&#39;: 0.8849459724950884},
 {&#39;train_loss&#39;: array(0.21713309, dtype=float32),
  &#39;train_acc&#39;: 0.9409381139489195},
 {&#39;train_loss&#39;: array(0.16108437, dtype=float32),
  &#39;train_acc&#39;: 0.9596021611001965}]</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="42" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" data-executionInfo="{&quot;status&quot;:&quot;ok&quot;,&quot;user&quot;:{&quot;userId&quot;:&quot;03923172999087610124&quot;,&quot;displayName&quot;:&quot;colabusage ido&quot;},&quot;user_tz&quot;:-120,&quot;elapsed&quot;:76049,&quot;timestamp&quot;:1671038959960}" id="t-grP9D3L3A7" data-outputId="d139b6e4-8b55-4c67-bccf-d92ef8167957">
<div class="sourceCode" id="cb112"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> test_loop(test_loader, pretrained_model, device, loss_fn)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>test accuracy = 0.3625170998632011, test loss = 3.586257
</code></pre>
</div>
</div>
<div class="cell markdown" id="RNs2TLd6TcDX">
<p>The model is misclassifying because of underfitting</p>
<p>Suggestions for improvement:</p>
<ol>
<li>Reduce learning rate</li>
<li>Extend the fully connected layer</li>
</ol>
</div>
<section id="experiment-summary-table" class="cell markdown" id="NptVnnnDXlkS">
<h2>Experiment Summary Table</h2>
</section>
<div class="cell markdown" id="cez4sE0RW8cF">
<p><img src="vertopal_adb52e36f4384793bcc5b99f2c560481/39dc817afc5be4d1cedab52af6d7e75eb5e339b4.png" alt="image.png" /></p>
</div>
<section id="final-report" class="cell markdown" id="-5XlRsJj8282">
<h1><strong>Final Report</strong></h1>
</section>
<div class="cell markdown" id="7QGU9BloAOgU">
<p>Before beginning your work on a model, it is important to first get to know the dataset you are going to work with. The cars dataset is pretty balanced with a similar amount of samples per class of varying sizes. This means that the most important step is to first resize the images to a common size. From the benchmarks published we can assess that training a model from scratch on this dataset yields mostly sub-par results. The most successful classifications are done via transfer training on pretrained models.</p>
<p>Our initial model implementation and training resulted in over fitting with very low validation accuracy and increasing loss. To improve upon this result we suggested various ways to tackle the problem, the important of which are: An addition of another fully connected layer before the prediction layer with a larger amount of in and out features can help the model to be more specific with its output which in turn will allow the last layer to classify the data based on more features. The addition of more dropouts in between layers will add another layer of randomization to our model which will "force" it to generalize the input data more instead of memorizing it.</p>
<p>When using inference time augmentation it is imperative to be acquainted with your dataset and the goal your model is trying to achieve. Applying augmentations that are relevant to your dataset is the key to making good use of this method.</p>
<p>Transfer learning could save a lot of time and yield good results but it isn't trivial. One must find a good learning fit, use fine tuning and select an appropriate pretrained model that fits the dataset. A model that was trained on data that contains similarities to our own data. The size of our dataset is also important for fine tuning the model to it.</p>
</div>
</body>
</html>
